---
title: 'Bayes Seminar'
author: 'Jens Klenke'
subtitle: "Seminar in Econometrics"
type: "Seminar Paper"
discipline: "VWL M.Sc."
date: "today"
studid: "3071594"
supervisor: "Christoph Hanck"
secondsupervisor: "NA"
semester: "5"
estdegree_emester: "Winter Term 2020"
deadline: "Jan. 17th 2020"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
linkcolor: black
urlcolor: black
citecolor: black
colorlinks: true
font: Times New Roman
fontsize: 12pt
geometry: lmargin = 5cm,rmargin = 2.5cm,tmargin = 2.5cm,bmargin = 2.5cm
biblio-files: references.bib
classoption: a4paper
---

<!-- % Template Version 1.1 -->

<!-- Git version -->

```{r , include=FALSE}
Sys.setlocale(locale = "English_United States.1252") ## English US Windows
knitr::opts_chunk$set(echo = TRUE)

#options(kableExtra.latex.load_packages = FALSE)


#### required packages ####
source("packages/packages.R")
```

# Introduction

In recent years, the \ac{LASSO} method of @tibshirani_regression_1996 has emerged as an alternative to ordinary least squares estimation. The success of the method is mainly due to its ability to perform both variable selection and estimation. As already Tibshirani pointed out in his original paper the standard \ac{LASSO} model can be interpreted as a linear regression with a Laplace prior. @park_bayesian_2008 where the first to implement the Bayesian l\ac{LASSO} >>using a conditional Laplace prior specification<<.

Our goal is to compare the result of the Bayesian \ac{LASSO} with normal \ac{LASSO} method and an ordinary least square estimation. The focus is particularly on the number of non-significant parameters in the linear model or, in case of the \acp{LASSO} the parameters equal to zero.  


Relevanz and structure


\newpage

# Theory of Bayesian inference  

The Bayesian (inference) statistics based on the Bayes' theorem for events.  

\begin{align}
\label{eq:bayes_theorem}
  P(A | B) = \dfrac{P (B | A) P(A)}{P(B)}
\end{align}

For Bayesian statistics the event theorem gets \eqref{eq:bayes_theorem} rewritten to apply it to densities. Where $\pi (\theta)$ is the prior distribution - which could be gained from prior research or knowledge, $f(y | \theta )$ is the likelihood function, and $\pi (\theta| y)$ is the posterior distribution, we then get the following. 

\begin{align}
\label{eq:bayes_dens}
  \pi (\theta | y) = \dfrac{f(y | \theta) \pi(\theta)}{f(y)}
\end{align}

From \eqref{eq:bayes_dens} the advantages and disadvantages of Bayesian statistics compared to frequentist statistics can directly be retrieved. One major advantage is that the Bayesian approach can account for prior knowledge and points out a philosophical difference to the frequentist approach - that the obtained data stands not alone. Another, key difference and advantage is that in the Bayesian world the computations are made with distributions and this leads to a better information level than just the computation of the first and second moment. The computation of distributions is also the greatest disadvantages or more neutral the biggest problem of the Bayesian approach because in high dimensional problems the computation takes a lot of times or is sometimes even not possible. A solution to that is that with newer and better computers it is possible to simulate the integrals with a \ac{MCMC} method.  [@ghosh_introduction_2006, p. 100] PAGE NUMBER!! 
\newpage

# Data description 

```{r include = FALSE, warning = TRUE}
# Data wrangling 

data_19 <- readr::read_csv(here::here('00_data/career_mode/players_19.csv'))
data_20 <- readr::read_csv(here::here('00_data/career_mode/players_20.csv'))

#### datsa manupilation ####

# creating a yearly variable dummy 

year <-as.data.frame(as.factor(c(rep(2019, nrow(data_19)),
         rep(2020, nrow(data_20)))))
colnames(year)[1] <- "year"

# hanging the different classes of the different data frames to merge these
data_19 <- data_19 %>%
  mutate_if(. , is.numeric, as.character) 

data_20 <- data_20 %>% 
  mutate_if(. , is.numeric, as.character)

# bind the data frames 
fifa_data <- bind_rows(data_19, data_20, .id = NULL)%>%
  bind_cols( year)%>%
  dplyr::select(-(player_traits:rb)) # getting rid of some unnecessary variables 


fifa_data <- fifa_data %>%
  mutate(age = as.numeric(age),                 #changing the class agian 
         value_eur = as.numeric(value_eur),
         log_value = log(value_eur),
         wage_eur = as.numeric(wage_eur),
         log_wage = log(wage_eur),
         height_cm = as.numeric(height_cm),
         weight_kg = as.numeric(weight_kg),
         overall = as.numeric(overall),
         potential = as.numeric(potential),
         player_positions = as.factor(player_positions),
         preferred_foot = as.factor(preferred_foot),
         international_reputation = as.factor(international_reputation),
         weak_foot = as.factor(weak_foot),
         skill_moves = as.factor(skill_moves),
         work_rate = as.factor(work_rate),
         body_type = as.factor(body_type),
         real_face  = as.logical(real_face),
         release_clause_eur = as.numeric(release_clause_eur),
         team_position = as.factor(team_position),
         contract_valid_until = as.numeric(contract_valid_until),
         pace = as.numeric(pace),
         shooting = as.numeric(shooting),
         passing = as.numeric(passing),
         defending = as.numeric(defending),
         dribbling = as.numeric(dribbling),
         gk_diving = as.numeric(gk_diving),                  
         gk_handling = as.numeric(gk_handling),                
         gk_kicking = as.numeric(gk_kicking),                
         gk_reflexes = as.numeric(gk_reflexes),            
         gk_speed = as.numeric(gk_speed),                
         gk_positioning = as.numeric(gk_positioning),
         nationality = as.factor(nationality),
         physic = as.numeric(physic),
  )%>% 
  filter( # deleting infinite values
    log_value > -Inf,
    log_wage > -Inf,
  )


vars <- c('log_wage', 'log_value', 'age', 'height_cm', 'weight_kg', 'overall',
          'potential', 'shooting', 'contract_valid_until', 'pace', 'shooting', 
          'passing', 'dribbling', 'defending')

train <- fifa_data %>%
  dplyr::filter(year == 2019)%>%
  dplyr::select(vars)%>%
  imputeTS::na_replace(., 0) # replace the NA's  (Bayesian) lasso can not handle NA's

train_y <- train%>%
  dplyr::select(log_value)

train_x <- train %>%
  dplyr::select(-log_value)


test <- fifa_data %>%
  dplyr::filter(year == 2020)%>%
  dplyr::select(vars)%>%
  imputeTS::na_replace(., 0)

test_y <- test%>%
  dplyr::select(log_value)

test_x <- test %>%
  dplyr::select(-log_value)


```

We collected the data from the online database platform _kaggel._ The dataset includes 6 years of data for all players who  were included in the soccer simulation game _FIFA_ from _EA Sports_. We decided to  keep the data for 2019 and 2020, only. The Data for 2019 contains `r length(fifa_data$value_eur[fifa_data$year == 2019])` datapoints which will be used for the estimation of the different models whereas the 2020 data with `r length(fifa_data$value_eur[fifa_data$year == 2020])` will be used to compare the quality of the models with an out of sample \ac{RMSE}. Both datasets consist of `r unique(dim(data_19)[[2]], dim(data_20)[[2]]) ` variables which will not all be included in the estimations. Some Variables are just an ID or different length of names and URLs. [@leone_fifa_2020]

A fundamental problem of the dataset consists as  goalkeepers are systematically rated differently than field players. Therefore, in the subcategories of the variable _overall_ all field player categories were assigned NAs for goalkeepers. Conversely, all field players have NAs in all goalkeeper categories. Because the algorithm of \ac{LASSO} in R cannot handle NAs they have been  set to zero for all models.


It is not very realistic that a fielder has no values in the goalkeeper categories and vice versa. However, it can be argued, at least for outfield players, that goalkeeper attributes play no role in determining market values. This argumentation does not seem to hold for goalkeepers, at least passing can be assumed to be an influential variable for the market value, because is an essential asset for the passing game if the goalkeeper has possession of the ball. Nevertheless, due to the lack of alternatives, all NAs have been replaced by Zero.


```{r data, echo = FALSE}

sum_var <- papeR::summarise( as.data.frame(fifa_data%>%dplyr::select(value_eur, wage_eur, overall, year, age, potential )), 
                   group = 'year', p_value = FALSE, quantil = FALSE )
  
sum_var <- sum_var[,1:((dim(sum_var)[2])-2)]
  
knitr::kable(sum_var, digits = 2, caption = '\\label{tab:sum} Summary of some important variables for the 2019 FIFA edition', format.args = list(decimal.mark = '.', big.mark = " "), col.names = c('', 'year', '', 'N', ' ', 'mean', 'sd' ), row.names = FALSE,  booktabs = TRUE, linesep = "")%>%
  kable_styling(latex_options = c("striped", "hold_position"))


```

As one can see in Table \ref{tab:sum} the differences between the editions for the most important variables are considerably small. For example,  from 2019 to 2020 the mean player _value_ (response variable) increased by `r scales::scientific(mean(fifa_data$value_eur[fifa_data$year ==2020]) - mean(fifa_data$value_eur[fifa_data$year ==2019]), digits = 3)` which is about `r round((mean(fifa_data$value_eur[fifa_data$year ==2020]) - mean(fifa_data$value_eur[fifa_data$year ==2019]))/ mean(fifa_data$value_eur[fifa_data$year ==2020])*100 ,2) ` per cent or `r round((mean(fifa_data$value_eur[fifa_data$year ==2020]) - mean(fifa_data$value_eur[fifa_data$year ==2019]))/ sd(fifa_data$value_eur[fifa_data$year ==2020]),2)` standard deviations. Similar results are observable for the probably most important righthand variables _wage_ and _overall_  with a difference in the means of `r round((mean(fifa_data$wage_eur[fifa_data$year ==2020]) -mean(fifa_data$wage_eur[fifa_data$year ==2019]))/ sd(fifa_data$wage_eur[fifa_data$year ==2020]),2)` and `r round((mean(fifa_data$overall[fifa_data$year ==2020]) -mean(fifa_data$overall[fifa_data$year ==2019]))/ sd(fifa_data$overall[fifa_data$year ==2020]),3)` standard deviations between 2019 and 2020.     

```{r , fig1, echo = FALSE, fig.cap = " \\label{fig:hist_1} Histograms of player values and log player values"}

plot_1 <- ggplot(data = fifa_data%>%filter(year == 2019), aes(value_eur))+
  geom_histogram(bins = 100, fill='blue',  alpha = 0.75)+
  labs(title = "Histogram of Player Values", x = "Value in Euro", y = "Quantity")+
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5), plot.margin=unit(c(1,1,1.5,1.2),"cm"))+
  scale_x_continuous( labels = scales::comma) 

plot_2 <- ggplot(data = fifa_data%>%filter(year == 2019), aes(log(value_eur)))+
  geom_histogram(bins = 100, fill='blue', alpha = 0.75)+
  labs(title = "Histogram of logarithmic Player Values", x = "Logarithmic Value in Euro", y = "Quantity")+
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5) , plot.margin=unit(c(1,1,1.5,1.2),"cm"))

plot_grid(plot_1, plot_2, align = "v", nrow = 2, rel_heights = c(1/4, 1/4, 1/2))
```


As can be seen for the variable value in Figure \ref{fig:hist_1}, this relatively strong right-skew is distributed, a similar pattern can be observed for the variable wage. Since we also estimate a linear model, and this often leads to non-normally distributed residuals, these were logarithmized.


\newpage

# Used Models

To compare the Bayesian \ac{LASSO} we will also analyse the data with a linear multivariate model, and the frequentist \ac{LASSO}. We will start with the linear model and then gradually modify the model equations respectively the condition for estimating the parameters. So that the coherences between the individual methods become clear.  

All three methods have the common assumption that the relationship is linear, at least in the parameters. The assumption seems stricter than it is at first, because the data can be manipulated in such a way that the relationship is linear after all. In our data, this was done by logarithmization.  

## Linear Model

The frequentist multivariate regression  model has the follwing model equation. 

\begin{align}
\label{eq:lm}
\pmb{Y = \beta_0 + X \beta} + \pmb{\epsilon}
\end{align}

Where $\pmb{y}$ is the $n \times 1$ response vector, $\pmb{X}$ is the $n \times p$ matrix of regressors and, $\pmb{\epsilon}$ is the $n \times 1$ vecotr of \ac{i.i.d.} errors with mean 0 and unknown variance $\sigma^2$. 

The coefficient will be estimated by the ordinary least square method, which means that $\pmb{\beta}$ should be chosen so that the _Euclidean norm_ $\left( || \mathbf{y - X\beta} ||_2 \right)$ is minimal. This yields in the conditon for the estimation of coefficients:

\begin{align}
\label{eq:lm_con}
 \hat{\pmb{\beta}} = \argmin_{ \pmb{\beta}} (\pmb{y - \beta_0 - X  \beta})^T (\pmb{y - \beta_0 - X  \beta})
\end{align}


## \acf{LASSO} 


In the \ac{LASSO} method the model equation is the same as the equation for the multivariate but the condition for the optimization of the estimators in equation \eqref{eq:lm_con} has an additional punishment term.  Which leads to the following optimazation.   

\begin{align}
\label{eq:la_con}
\hat{\pmb{\beta}} = \argmin_{\pmb{\beta}}  \left( \pmb{y - X \beta} \right)^T \left( \pmb{y - X \beta} \right) + \lambda \sum_{i = 1}^{p} |\beta_j|
\end{align}


for some $\lambda \geq 0$. This method is also often referred to as $L_1$ -penalized least squares estimation.  

Already in his original paper @tibshirani_regression_1996 has pointed out the possibility that his methods can also be interpreted in a Bayesian way. The LASSO estimates can be considered as posterior mode estimates with a double-exponential Laplace prior.


## Bayesian Lasso

@park_bayesian_2008 considered a fully Bayesian approach using a conditional Laplace prior of the form 

\begin{align} 
\label{eq:la_bay_prior}
 \pi \left( \pmb{\beta} | \sigma^2 \right)   = \prod_{j = 1}^{p} \dfrac{\lambda}{2 \sqrt{\sigma^e}} e^{\dfrac{- \lambda |\beta_j |}{\sqrt{\sigma^2}}} 
\end{align}

[@park_bayesian_2008]

### Gibbs Sampler and 

The Gibbs Sampler is a special case of an \ac{MCMC} algorithm, which is useful to approximate the combianed distribution of two or more regressors in a multidemsinoal problem. 

The algortithm tries to find the approximate joint distribution and therefore the algortihm runs through the subvectors $\beta$ and draws ach subset conditional on all other values. [@gelman_bayesian_2004]


Bayesian \ac{LASSO} the Gibbs sampler in the __monomvn__ package in __R__ [@gramacy_monomvn_2019] samples from the following representation of the Laplace distribution. @andrews_scale_1974

\begin{align} 
\label{eq:gibbs}
  \dfrac{a}{2}e^{-a |z|} = \int_{0}^{\infty} \dfrac{1}{2 \sqrt{\sigma^2}} e^{ -z^2 / (2s)} \; \dfrac{a^2}{2} e^{ -a^2 s /2} ds, \qquad a > 0 
\end{align}



### The full Model specification

The full model has the following hierarchical representation

\begin{align*}
  \pmb{y|\mu}, \pmb{X}, \pmb{\beta}, \sigma^2 & \sim N_n (\mu \pmb{1}_n + \pmb{X \beta}, \sigma^2  \pmb{I}_n)   \nonumber\\
  \pmb{\beta} | \sigma^2, \tau^2_1 , \ldots , \tau^2_p & \sim N_p (\pmb{A}^{-1} \pmb{X}^T \tilde{\pmb{y}}  , \sigma^2 \pmb{A}^{-1}) \qquad \pmb{A} = \pmb{X}^T \pmb{X} + \pmb{D}^{-1}_{\tau} \nonumber \\
  \pmb{D}_{\tau} & = diag(\tau^2_1 , \ldots , \tau^2_p) \nonumber \\ 
  \sigma^2, \tau^2_1 , \ldots , \tau^2_p & \sim \pi \left( \sigma^2 \right) d \sigma^2 \prod_{j = 1}^{p} \dfrac{\lambda^2}{2}e^{- \lambda^2 \tau^2_j /2} d \tau^2_j \nonumber \\
  \sigma^2, \tau^2_1 , \ldots , \tau^2_p & > 0 \nonumber \\
\end{align*}

If $\tau^2_1 , \ldots , \tau^2_p$ gets integreted out of the conditional prior on $\pmb{\beta}$ , we get the form of \eqref{eq:la_bay_prior}. For $\sigma^2$ the inverse-gamma function of the form $\pi \left( \sigma^2 \right) = \dfrac{1}{\sigma^2}$ was implemented in the __monomvn__ package.     


\newpage
# Estimation and Results of the Models  


To compare the performances of the models all three models got, obviously, estimated with the same regressors.  We included as righthand variables: _log_wage_ , _age_, _height_cm_, _weight_kg_, _overall_, _potential_, _shooting_, _contract_valid_until_, _pace_, _passing_, _dribbling_,  and _defending_, so we have `r dim(train_x)[2]` to predict the response variable _log_value_. 

## Linear Model

\FloatBarrier
```{r structure and lm, echo = FALSE}

# the estimation of the models are in a seperate file, so that the compilation does not take to long, the models were stored in an additional file

######### linear model

#source("01_code/lm.R")

load(file = here::here('04_output/fit_lm.RData'))


knitr::kable(summary(fit_lm)$coef, digits = 4, caption = '\\label{tab:sum_lm} Summary of the linear model', format.args = list(decimal.mark = '.', big.mark = " "),   booktabs = TRUE, linesep = "")%>%
  kable_styling(latex_options = c("striped", "hold_position"))

#sum(summary(fit_lm)$coef[,4] > 0.05)
```
\FloatBarrier

In Table \ref{tab:sum_lm} one can see that only `r sum(summary(fit_lm)$coef[,4] > 0.05)` parameter is not significant to a 5 per cent level. The variable _overall_ has, naturally, the biggest (positive) impact on the _log_value (value)_, whereas _age_ has the biggest negative effect. 

Table \ref{tab:sum_lm} also shows that some coefficients are relatively small but still significant. However, a general problem with \ac{OLS} estimation is that with increasing sample size, many "coefficients" become significant. This is because the standard errors become smaller with increasing N, the t-statistic becomes larger, and the p-value smaller. These coefficients (e.g.: _pace_, or _passing_ ) could be zero in the \ac{LASSO} estimation because of the punishment term.   

## \acf{LASSO}


```{r , echo = FALSE}
 #### freq LASSO ####

 #source("01_code/glmnet.R")

 load(file = here::here('04_output/glmnet_lm.RData'))

 glmnet_coef <- as.matrix(coef(glmnet_fit))

 glmnet_coef_k <- round(glmnet_coef,6)
 glmnet_coef_k[glmnet_coef_k == 0] <- '-'
```

For the frequentists \ac{LASSO} we used the __cv.glmnet__ cross-validation function from the __glmnet__ package with 100 folds to gain $\lambda$. A $\lambda$ of `r round(LA_all$lambda.min,5)` minimized the mean cross-validated error. However, we used a lambda of  `r round(LA,5)` which is the largest $\lambda$ such that error is still within one standard error of the minimum.     @hastle_glmnet_2019


```{r , echo = FALSE}

 knitr::kable(glmnet_coef_k, digits = 6, caption = '\\label{tab:sum_lasso} Summary of the LASSO ', format.args = list(decimal.mark = '.', big.mark = " "),  col.names = c('Estimate'),  booktabs = TRUE, linesep = "")%>%
   kable_styling(latex_options = c("striped", "hold_position"))

# sum(glmnet_coef == 0)
```

  
As one can see in Table \ref{tab:sum_lasso} there are significant differences to the linear system of equations. Lasso has shrunk `r sum(glmnet_coef == 0)` parameters so much that they are no longer included in the model equation.  It may be particularly noticeable, because it seems contra intuitive and it had the biggest impact in the linear model from the `r sum(glmnet_coef == 0)` excluded parameters, that the variable _contract_valif_until_ is also not represented in the model.  

Since LASSO does not only estimate regressors, but also selects them, no significance tests are needed 


\newpage
## Bayesian Lasso 

### Settings of the hyperparameters 

In the __blasso__ function of the __R__ package __monomvn__ it is possible to set the hyperparameters $\lambda$, for the penalty term, and  $\alpha \,  \text{and} \, \beta$, which are the shape and rate parameter for the prior. The $\lambda$ is in our case an empirical parameter which will be approximate trhough an updating Gibbs sampler. The algorithm uses the parameter of the previous sample. So iteration $k$ uses the Gibbs sampler with hyperparmeter $k-1$.
For the frequentists \ac{LASSO} the $\lambda$-parameter was `r round(LA, 5)`, so we decided to set  $\lambda = 10$,  since the first $25  \%$ of the \ac{MCMC} are not used for the estimation and the sampler convergence rather quickly.  [@gramacy_monomvn_2019] 

\FloatBarrier
\begin{align*} 
  \lambda^{k} = \sqrt{\dfrac{2p}{\displaystyle \sum_{j = 1}^{p} E_{\lambda^{(k-1)}} [\tau_j^2| \pmb{y}] }}
\end{align*}
\FloatBarrier
The expectations are replaced with averages from the previous Gibbs sampler. As @park_bayesian_2008 has shown any non extrem starting value for $\lambda$ can be used. In the first setting we did not pass any parameters for $\alpha \, \text{and} \, \beta$. 

\FloatBarrier

```{r , echo = FALSE}
 
 ### bayes LASSO ###
#source("01_code/bayes.R")
load(file = here::here('04_output/bayes_lasso_fit.RData'))

coef_lasso <- as.data.frame(cbind(beta1 = bayes_lasso_fit$beta[, "b.1"],
                                  beta2 = bayes_lasso_fit$beta[, "b.2"],
                                   beta3 = bayes_lasso_fit$beta[, "b.3"],
                                   beta4 = bayes_lasso_fit$beta[, "b.4"],
                                   beta5 = bayes_lasso_fit$beta[, "b.5"],
                                   beta6 = bayes_lasso_fit$beta[, "b.6"],
                                   beta7 = bayes_lasso_fit$beta[, "b.7"],
                                   beta8 = bayes_lasso_fit$beta[, "b.8"],
                                   beta9 = bayes_lasso_fit$beta[, "b.9"],
                                   beta10 = bayes_lasso_fit$beta[, "b.10"],                                                                       beta11 = bayes_lasso_fit$beta[, "b.11"],
                                   beta12 = bayes_lasso_fit$beta[, "b.12"],
                                   variance = bayes_lasso_fit$s2,
                                   lambda.square = bayes_lasso_fit$lambda2))

 coef_lasso_hyper <- as.data.frame(cbind(beta1 = bayes_lasso_fit_hyper$beta[, "b.1"],
                                   beta2 = bayes_lasso_fit_hyper$beta[, "b.2"],
                                   beta3 = bayes_lasso_fit_hyper$beta[, "b.3"],
                                   beta4 = bayes_lasso_fit_hyper$beta[, "b.4"],
                                   beta5 = bayes_lasso_fit_hyper$beta[, "b.5"],
                                   beta6 = bayes_lasso_fit_hyper$beta[, "b.6"],
                                   beta7 = bayes_lasso_fit_hyper$beta[, "b.7"],
                                   beta8 = bayes_lasso_fit_hyper$beta[, "b.8"],
                                   beta9 = bayes_lasso_fit_hyper$beta[, "b.9"],
                                   beta10 = bayes_lasso_fit_hyper$beta[, "b.10"],
                                   beta11 = bayes_lasso_fit_hyper$beta[, "b.11"],
                                   beta12 = bayes_lasso_fit_hyper$beta[, "b.12"],
                                   variance = bayes_lasso_fit_hyper$s2,
                                   lambda.square = bayes_lasso_fit_hyper$lambda2))

 BA <- cbind(colMedians(coef_lasso),
 matrixStats::colQuantiles(as.matrix(coef_lasso), probs = c(0.025, 0.975)))
 colnames(BA)[1] <- 'median'
 rownames(BA)[1:12] <- rownames(glmnet_coef_k)[2:13]


 knitr::kable(BA, digits = 6, caption = '\\label{tab:sum_bay} Summary of the Bayesian LASSO ', format.args = list(decimal.mark = '.', big.mark = " "),  booktabs = TRUE, linesep = "")%>%
   kable_styling(latex_options = c("striped"))#, "hold_position"))

# sum(BA[,2] * BA[,3] <= 0)

```

\FloatBarrier
As one can see in Tabel \ref{tab:sum_bay} the _median_ for all regressors are unequal to zero, whereas for the frequentist \ac{LASSO} we have `r sum(glmnet_coef == 0)` coefficient which are directly ecluded from the model, e.g. zero. 

```{r tabel_bayes_1, echo = FALSE}
 HY <- cbind(colMedians(coef_lasso_hyper),
 matrixStats::colQuantiles(as.matrix(coef_lasso_hyper), probs = c(0.025, 0.975)))
 colnames(HY)[1] <- 'median'
 rownames(HY)[1:12] <- rownames(glmnet_coef_k)[2:13]
 
# knitr::kable(HY, digits = 6, caption = '\\label{tab:sum_bay_hy} Summary of the  Bayessian LASSO with hyperpriors', format.args = list(decimal.mark = '.', big.mark = " "),  booktabs = TRUE, linesep = "")%>%
 #  kable_styling(latex_options = c("striped", "hold_position"))

# sum(HY[,2] * HY[,3] <= 0)
```

However, it is unlikely that for multidimensional bayesian model the median for a parameter is zero, since the computation depends on a Gibbs sampler. If we instead look at the 95 \%  credible interval we finde that `r sum(BA[,2] * BA[,3] <= 0)` of these intervall include the zero.  






\newpage
# Residual Analysis, \acf{RMSE} and 'Sensitive Analysis' 

The next step is to compare the quality of the model. First we will take a look at the (distribution of the) residuals and after that we will calculate the out-of-sample \ac{RMSE} for the 2020 _FIFA_ data set. 



```{r RMSE, echo = FALSE}
rmse_lm <-modelr::rmse(fit_lm, test)

##own RMSE Function
own_rmse  <-  function(y_hat, y) {
   rm_se <- sqrt(sum( ( (y_hat - y)^2)/length(y_hat) ))
   return(rm_se)
}


### predicting y_hat LASSO

#prediction LASSO

y_hat_freq_las <- rep(glmnet_coef[1],length(test$log_wage))+
  glmnet_coef[which(row.names(glmnet_coef) == 'log_wage')]*test$log_wage+ 
  glmnet_coef[which(row.names(glmnet_coef) == 'age')]*test$age + 
  glmnet_coef[which(row.names(glmnet_coef) == 'height_cm')]*test$height_cm +
  glmnet_coef[which(row.names(glmnet_coef) == 'weight_kg')]*test$weight_kg +
  glmnet_coef[which(row.names(glmnet_coef) == 'overall')]*test$overall +
  glmnet_coef[which(row.names(glmnet_coef) == 'potential')]*test$potential +
  glmnet_coef[which(row.names(glmnet_coef) == 'shooting')]*test$shooting +
  glmnet_coef[which(row.names(glmnet_coef) == 'contract_valid_until')]*test$contract_valid_until +  
  glmnet_coef[which(row.names(glmnet_coef) == 'pace')]*test$pace +
  glmnet_coef[which(row.names(glmnet_coef) == 'passing')]*test$passing +
  glmnet_coef[which(row.names(glmnet_coef) == 'dribbling')]*test$dribbling +    
  glmnet_coef[which(row.names(glmnet_coef) == 'defending')]*test$defending   

rmse_freq_las <- own_rmse(y_hat_freq_las, test$log_value)



###predicting Bayesian Lasso

y_hat_bay_las <- rep(median(bayes_lasso_fit$mu), length(test$log_wage))+
  BA[1,1]*test$log_wage +
  BA[2,1]*test$age +
  BA[3,1]*test$height_cm +
  BA[4,1]**test$weight_kg +
  BA[5,1]*test$overall +
  BA[6,1]*test$potential +
  BA[7,1]*test$shooting +
  BA[8,1]*test$contract_valid_until +  
  BA[9,1]*test$pace +
  BA[10,1]*test$passing +
  BA[11,1]*test$dribbling +   
  BA[12,1]*test$defending   

rmse_bay_las <- own_rmse(y_hat_bay_las , test$log_value)

###predicting Bayesian Lasso with hyper perameter

y_hat_bay_las_hy <- rep(median(bayes_lasso_fit_hyper$mu), length(test$log_wage))+
  HY[1,1]*test$log_wage +
  HY[2,1]*test$age +
  HY[3,1]*test$height_cm +
  HY[4,1]**test$weight_kg +
  HY[5,1]*test$overall +
  HY[6,1]*test$potential +
  HY[7,1]*test$shooting +
  HY[8,1]*test$contract_valid_until +  
  HY[9,1]*test$pace +
  HY[10,1]*test$passing +
  HY[11,1]*test$dribbling +   
  HY[12,1]*test$defending   

rmse_bay_las_hyp <- own_rmse(y_hat_bay_las_hy , test$log_value)

load( file = here::here('04_output/fitted.RData'))

res_bayes <- (train_y - fitted_va_bayes)
colnames(res_bayes) <- 'resdiuals'

```


## Residual Analysis
Residues are defined as the difference between the predicted value of the model and the actual value. As you can see from equation \eqref{eq:res}, negative residuals mean that the model overestimates the value and positive residuals mean that the model underestimates the value.   [@hayashi_econometrics_2000, p. 16]

\begin{align} 
\label{eq:res}
  \epsilon =  y_i - \hat{y}_i =  y_i - (  \beta_0  + \pmb{\beta}_i \pmb{X})
\end{align}

One crucial assumption of the linear regression is that the residuals are normally distributed with mean $0$ and constant variance $\sigma^2$. 

```{r, , fig2, echo = FALSE, fig.cap = " \\label{fig:res_bay} Plot of the Residuals vs Fitted Values for the Bayesian LASSO",  out.width = '100%',  fig.align = "left"}
  knitr::include_graphics(here::here('04_output/pl_bay_res.png'), auto_pdf = FALSE)
```

In Figure \ref{fig:res_bay} the residuals versus the fitted values were plotted and it appears that several assumptions are violated, on a first glance. On the other hand there seem to be clusters of different variances. The variance in the range between 10 and 13 seems to be larger than the variance between 15 and 18, which could be a sign for heteroscedasticity. 

Furthermore, the model seems to have a systematic estimation error at high values, all values above 16 all residuals are negative, i.e. the model overestimates the value of the players. Generally it can be said that a pattern can be recognized and the residuals do not appear distributed independently of each other. 


```{r, , fig_dens_2, echo = FALSE, fig.cap = " \\label{fig:den_res_bay} Distribution of the Residuals of the Bayesian LASSO",  out.width = '100%',  fig.align = "left"}
   knitr::include_graphics(here::here('04_output/pl_bay_res_dens.png'), auto_pdf = FALSE)
```

The distribution of the residuals also does not seem to be normally distributed with a mean value of 0. In Figure \ref{fig:den_res_bay} it seems os that the left tail is distributed much longer and wider than the right tail.

The empirical mean of the residuals is `r mean(as.matrix(res_bayes))`, which is significant at a one percent level with a t-static of `r round(t.test(as.matrix(res_bayes))$statistic, 2) ` and a p-value of `r formatC( t.test(as.matrix(res_bayes))$p.value, format = "e", digits = 2)`.    












Verteilung der Residuen plotten

Structur bruch? 
Varianz
wenig hohe werte systemastisch unterschätzt ausreißer 







## \ac{RMSE}


 As mentioned in the first Section we will use the data from _2020_ as test data.  

We used 

The measurment to compare the results will be the \acf{RMSE}, which is definied as:

\begin{align*} 
  RMSE = \sqrt{ \dfrac{ \displaystyle \sum_{i = 1}^{N} \left( \hat{y}_i  - y_i \right)^2} {N}    }
\end{align*}



<!-- hyper -->

\FloatBarrier
```{r sensity , echo = FALSE }
 HY <- cbind(colMedians(coef_lasso_hyper),
 matrixStats::colQuantiles(as.matrix(coef_lasso_hyper), probs = c(0.025, 0.975)))
 colnames(HY)[1] <- 'median'
 rownames(HY)[1:12] <- rownames(glmnet_coef_k)[2:13]
  knitr::kable(HY, digits = 6, caption = '\\label{tab:sum_bay_hy_1} Summary of the  Bayessian LASSO with hyperpriors', format.args = list(decimal.mark = '.', big.mark = " "),  booktabs = TRUE, linesep = "")%>%
   kable_styling(latex_options = c("striped", "hold_position"))

# sum(HY[,2] * HY[,3] <= 0)
```
\FloatBarrier


\newpage
# Discussion and further research 
Struckturbruch
LaSt Part

\newpage



# Appendix

$\quad$

```{r, , fig3, echo = FALSE, fig.cap = " \\label{fig:res_lm} Plot of the Residuals vs Fitted Values for the Linear Model",  out.width = '100%',  fig.align = "left"}
  knitr::include_graphics(here::here('04_output/pl_lin_res.png'), auto_pdf = FALSE)
```


```{r, , fig4, echo = FALSE, fig.cap = " \\label{fig:res_lasso} Plot of the Residuals vs Fitted Values for the LASSO Model",  out.width = '100%',  fig.align = "left"}
  knitr::include_graphics(here::here('04_output/pl_las_res.png'), auto_pdf = FALSE)
```



\newpage
