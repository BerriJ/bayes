---
title: 'Analysing FIFA Data with the Bayesian LASSO'
author: 'Jens Klenke'
subtitle: 'Seminar in Econometrics'
type: "Term Paper"
discipline: "VWL M.Sc."
date: "today"
studid: "3071594"
supervisor: "Christoph Hanck"
secondsupervisor: "NA"
semester: "5"
estdegree_emester: "Winter Term 2020"
deadline: "Jan. 17th 2020"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
linkcolor: black
urlcolor: black
citecolor: black
colorlinks: true
font: Times New Roman
fontsize: 12pt
geometry: lmargin = 5cm,rmargin = 2.5cm,tmargin = 2.5cm,bmargin = 2.5cm
biblio-files: references.bib
classoption: a4paper
---

<!-- % Template Version 1.1 -->

<!-- Git version -->

```{r , include=FALSE}
Sys.setlocale(locale = "English_United States.1252") ## English US Windows
knitr::opts_chunk$set(echo = TRUE)

#options(kableExtra.latex.load_packages = FALSE)


#### required packages ####
source("packages/packages.R")
```

# Introduction

In recent years, the \ac{LASSO} method by @tibshirani_regression_1996 has emerged as an alternative to ordinary least squares estimation. The success of the method is mainly due to its ability to perform both variable selection and estimation. As Tibshirani already pointed out in his original paper the standard \ac{LASSO} model can be interpreted as a linear regression with a Laplace prior. @park_bayesian_2008 were the first to implement the Bayesian \ac{LASSO} >>using a conditional Laplace prior specification<<.

Our goal is to compare the results of the Bayesian \ac{LASSO} with the normal \ac{LASSO} method and an ordinary least square estimation. The focus is particularly on the number of non-significant parameters in the linear model or, in case of the \acp{LASSO} the parameters equal to zero. To compare the different methods we will use the FIFA data sets from 2019 and 2020.   

Bayesian statistics has grown very strongly in recent years. This is due to the improvement of computers which have made the calculation possible in the first place. Now they make it possible to calculate these methods in increasing numbers and in even shorter time. In view of these changes, it seems logical that the Bayesian LASSO should be used more and more often, since this method can select variables and also estimate variables by means of distributions. This leads to the fact that the advantages of LASSO and Bayesian statistics come together. Nevertheless, the Bayesian methods still take much longer to calculate than the frequentist methods. Therefore, in this paper we want to analyze whether the Bayesian LASSO has an advantage over the frequentist LASSO. 

In the first chapter we will present the general approach of Bayesian statistics. Afterwards, we will describe the data  basis and preparation. The next step is to introduce the methods used and to present the results. Before we will draw a conclusion, the models will be analyzed with respect to their residuals. 

\newpage

# Theory of Bayesian inference  

The Bayesian (inference) statistics based on the Bayes' theorem for events.  

\begin{align}
\label{eq:bayes_theorem}
  P(A | B) = \dfrac{P (B | A) P(A)}{P(B)}
\end{align}

For Bayesian statistics the event theorem \eqref{eq:bayes_theorem} gets rewritten to apply it to densities. 

Where $\pi (\theta)$ is the prior distribution - which could be gained from prior research or knowledge, $f(y | \theta )$ is the likelihood function, and $\pi (\theta| y)$ is the posterior distribution, which then yields in the following equation:


\begin{align}
\label{eq:bayes_dens}
  \pi (\theta | y) = \dfrac{f(y | \theta) \pi(\theta)}{f(y)}
\end{align}

From equation \eqref{eq:bayes_dens} the advantages and disadvantages of Bayesian statistics compared to frequentist statistics can directly be retrieved. One major advantage is that the Bayesian approach can account for prior knowledge and points out a philosophical difference to the frequentist approach - that the obtained data stands not alone. 

Another key difference and advantage is that in the Bayesian statistic the computations are made with distributions and this leads to a better information level than just the computation of the first and second moment. 

The computation with distributions is also the greatest disadvantage or -  more neutral - the biggest problem of the Bayesian approach because in high dimensional problems the computation takes a lot of time and in some cases it is even impossible. A solution to that is that with newer and better computers it is possible to simulate the integrals with a \ac{MCMC} method.  [@ghosh_introduction_2006, p. 35 ff.] 


\newpage

# Data description 

```{r include = FALSE, warning = TRUE}
# Data wrangling 

data_19 <- readr::read_csv(here::here('00_data/career_mode/players_19.csv'))
data_20 <- readr::read_csv(here::here('00_data/career_mode/players_20.csv'))

#### datsa manupilation ####

# creating a yearly variable dummy 

year <-as.data.frame(as.factor(c(rep(2019, nrow(data_19)),
         rep(2020, nrow(data_20)))))
colnames(year)[1] <- "year"

# hanging the different classes of the different data frames to merge these
data_19 <- data_19 %>%
  mutate_if(. , is.numeric, as.character) 

data_20 <- data_20 %>% 
  mutate_if(. , is.numeric, as.character)

# bind the data frames 
fifa_data <- bind_rows(data_19, data_20, .id = NULL)%>%
  bind_cols( year)%>%
  dplyr::select(-(player_traits:rb)) # getting rid of some unnecessary variables 


fifa_data <- fifa_data %>%
  mutate(age = as.numeric(age),                 #changing the class agian 
         value_eur = as.numeric(value_eur),
         log_value = log(value_eur),
         wage_eur = as.numeric(wage_eur),
         log_wage = log(wage_eur),
         height_cm = as.numeric(height_cm),
         weight_kg = as.numeric(weight_kg),
         overall = as.numeric(overall),
         potential = as.numeric(potential),
         player_positions = as.factor(player_positions),
         preferred_foot = as.factor(preferred_foot),
         international_reputation = as.factor(international_reputation),
         weak_foot = as.factor(weak_foot),
         skill_moves = as.factor(skill_moves),
         work_rate = as.factor(work_rate),
         body_type = as.factor(body_type),
         real_face  = as.logical(real_face),
         release_clause_eur = as.numeric(release_clause_eur),
         team_position = as.factor(team_position),
         contract_valid_until = as.numeric(contract_valid_until),
         pace = as.numeric(pace),
         shooting = as.numeric(shooting),
         passing = as.numeric(passing),
         defending = as.numeric(defending),
         dribbling = as.numeric(dribbling),
         gk_diving = as.numeric(gk_diving),                  
         gk_handling = as.numeric(gk_handling),                
         gk_kicking = as.numeric(gk_kicking),                
         gk_reflexes = as.numeric(gk_reflexes),            
         gk_speed = as.numeric(gk_speed),                
         gk_positioning = as.numeric(gk_positioning),
         nationality = as.factor(nationality),
         physic = as.numeric(physic),
  )%>% 
  filter( # deleting infinite values
    log_value > -Inf,
    log_wage > -Inf,
  )


vars <- c('log_wage', 'log_value', 'age', 'height_cm', 'weight_kg', 'overall',
          'potential', 'shooting', 'contract_valid_until', 'pace', 'shooting', 
          'passing', 'dribbling', 'defending')

train <- fifa_data %>%
  dplyr::filter(year == 2019)%>%
  dplyr::select(vars)%>%
  imputeTS::na_replace(., 0) # replace the NA's  (Bayesian) lasso can not handle NA's

train_y <- train%>%
  dplyr::select(log_value)

train_x <- train %>%
  dplyr::select(-log_value)


test <- fifa_data %>%
  dplyr::filter(year == 2020)%>%
  dplyr::select(vars)%>%
  imputeTS::na_replace(., 0)

test_y <- test%>%
  dplyr::select(log_value)

test_x <- test %>%
  dplyr::select(-log_value)


```

We collected the data from the online database platform _kaggel._ The dataset includes 6 years of data for all players who  were included in the soccer simulation game _FIFA_ from _EA Sports_. We decided to  keep the data for 2019 and 2020, only. The Data for 2019 contains `r length(fifa_data$value_eur[fifa_data$year == 2019])` datapoints which will be used for the estimation of the different models whereas the 2020 data with `r length(fifa_data$value_eur[fifa_data$year == 2020])` will be used to compare the quality of the models with an out of sample \ac{RMSE}. Both datasets consist of `r unique(dim(data_19)[[2]], dim(data_20)[[2]]) ` variables which will not all be included in the estimations. Some Variables are just an ID or different length of names and URLs. [@leone_fifa_2020]

A fundamental problem of the dataset consists as  goalkeepers are systematically rated differently than field players. Therefore, in the subcategories of the variable _overall_ all field player categories were assigned NAs for goalkeepers. Conversely, all field players have NAs in all goalkeeper categories. Because the algorithm of \ac{LASSO} in R cannot handle NAs they have been  set to zero for all models.


It is not very realistic that a fielder has no values in the goalkeeper categories and vice versa. However, it can be argued, at least for outfield players, that goalkeeper attributes play no role in determining market values. This argumentation does not seem to hold for goalkeepers, at least passing can be assumed to be an influential variable for the market value, because is an essential asset for the passing game if the goalkeeper has possession of the ball. Nevertheless, due to the lack of alternatives, all NAs have been replaced by Zero.


```{r data, echo = FALSE}

sum_var <- papeR::summarise( as.data.frame(fifa_data%>%dplyr::select(value_eur, wage_eur, overall, year, age, potential )), 
                   group = 'year', p_value = FALSE, quantil = FALSE )
  
sum_var <- sum_var[,1:((dim(sum_var)[2])-2)]
  
knitr::kable(sum_var, digits = 2, caption = '\\label{tab:sum} Summary of some important variables for the 2019 FIFA edition', format.args = list(decimal.mark = '.', big.mark = " "), col.names = c('', 'year', '', 'N', ' ', 'mean', 'sd' ), row.names = FALSE,  booktabs = TRUE, linesep = "")%>%
  kable_styling(latex_options = c("striped", "hold_position"))


```


As one can see in Table \ref{tab:sum} the differences between the editions for the most important variables are considerably small. 

For example,  from 2019 to 2020 the mean player _value_ (response variable) increased by `r scales::scientific(mean(fifa_data$value_eur[fifa_data$year ==2020]) - mean(fifa_data$value_eur[fifa_data$year ==2019]), digits = 3)` which is about `r round((mean(fifa_data$value_eur[fifa_data$year ==2020]) - mean(fifa_data$value_eur[fifa_data$year ==2019]))/ mean(fifa_data$value_eur[fifa_data$year ==2020])*100 ,2) ` per cent or `r round((mean(fifa_data$value_eur[fifa_data$year ==2020]) - mean(fifa_data$value_eur[fifa_data$year ==2019]))/ sd(fifa_data$value_eur[fifa_data$year ==2020]),2)` standard deviations. Similar results are observable for the probably most important righthand variables _wage_ and _overall_  with a difference in the means of `r round((mean(fifa_data$wage_eur[fifa_data$year ==2020]) -mean(fifa_data$wage_eur[fifa_data$year ==2019]))/ sd(fifa_data$wage_eur[fifa_data$year ==2020]),2)` and `r round((mean(fifa_data$overall[fifa_data$year ==2020]) -mean(fifa_data$overall[fifa_data$year ==2019]))/ sd(fifa_data$overall[fifa_data$year ==2020]),3)` standard deviations between 2019 and 2020.     

```{r , fig1, echo = FALSE, fig.cap = " \\label{fig:hist_1} Histograms of player values and log player values"}

plot_1 <- ggplot(data = fifa_data%>%filter(year == 2019), aes(value_eur))+
  geom_histogram(bins = 100, fill='blue',  alpha = 0.75)+
  labs(title = "Histogram of Player Values", x = "Value in Euro", y = "Quantity")+
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5), plot.margin=unit(c(1,1,1.5,1.2),"cm"))+
  scale_x_continuous( labels = scales::comma) 

plot_2 <- ggplot(data = fifa_data%>%filter(year == 2019), aes(log(value_eur)))+
  geom_histogram(bins = 100, fill='blue', alpha = 0.75)+
  labs(title = "Histogram of logarithmic Player Values", x = "Logarithmic Value in Euro", y = "Quantity")+
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5) , plot.margin=unit(c(1,1,1.5,1.2),"cm"))

plot_grid(plot_1, plot_2, align = "v", nrow = 2, rel_heights = c(1/4, 1/4, 1/2))
```


As can be seen for the variable _value_ in Figure \ref{fig:hist_1}, this relatively strong right-skew is distributed, a similar pattern can be observed for the variable wage. Since we also have estimate a linear model, and this often leads to non-normally distributed residuals, these were logarithmized.


\newpage

# Used Models

To compare the Bayesian \ac{LASSO} we will also analyze the data with a linear multivariate model, and the frequentist \ac{LASSO}. We will start with the linear model and then gradually modify the model equations respectively the condition for estimating the parameters. So that the coherences between the individual methods become clear.  

All three methods have the common assumption that the relationship is linear, at least in the parameters. Inatially, the assumption seems stricter than it is, because the data can be manipulated in such a way that the relationship is linear after all. In our data this was done by logarithmization.  

## Linear Model

The frequentist multivariate regression  model has the following model equation. 

\begin{align}
\label{eq:lm}
\pmb{Y = \beta_0 + X \beta} + \pmb{\epsilon}
\end{align}

Where $\pmb{y}$ is the $n \times 1$ response vector, $\pmb{X}$ is the $n \times p$ matrix of regressors and, $\pmb{\epsilon}$ is the $n \times 1$ vector of \ac{i.i.d.} errors with mean 0 and unknown but constant variance $\sigma^2$. 

The coefficient will be estimated by the ordinary least square method, which means that $\pmb{\beta}$ should be chosen so that the _Euclidean norm_ $\left( || \mathbf{y - X\beta} ||_2 \right)$ is minimal. This yields in the condition for the estimation of coefficients:

\begin{align}
\label{eq:lm_con}
 \hat{\pmb{\beta}} = \argmin_{ \pmb{\beta}} (\pmb{y - \beta_0 - X  \beta})^T (\pmb{y - \beta_0 - X  \beta})
\end{align}


## \acf{LASSO} 


In the \ac{LASSO} method the model equation is the same as the equation for the multivariate but the condition for the optimization of the estimators in equation \eqref{eq:lm_con} has an additional punishment term.  Which leads to the following optimization.   

\begin{align}
\label{eq:la_con}
\hat{\pmb{\beta}} = \argmin_{\pmb{\beta}}  \left( \pmb{y - X \beta} \right)^T \left( \pmb{y - X \beta} \right) + \lambda \sum_{i = 1}^{p} |\beta_j|
\end{align}


for some $\lambda \geq 0$. This method is also often referred to as $L_1$ -penalized least squares estimation.  

Already in his original paper @tibshirani_regression_1996 has pointed out the possibility that his methods can also be interpreted in a Bayesian way. The LASSO estimates can be considered as posterior mode estimates with a double-exponential Laplace prior.


## Bayesian Lasso

@park_bayesian_2008 considered a fully Bayesian approach using a conditional Laplace prior of the form 

\begin{align} 
\label{eq:la_bay_prior}
 \pi \left( \pmb{\beta} | \sigma^2 \right)   = \prod_{j = 1}^{p} \dfrac{\lambda}{2 \sqrt{\sigma^e}} e^{\dfrac{- \lambda |\beta_j |}{\sqrt{\sigma^2}}} 
\end{align}


The analysis of the FIFA data set is a multidimensional problem and therefore in Bayesian statistics this is an analysis that only works with a hierarchical model. The solution cannot be calculated directly but has to be solved with the Gibbs sampler.

### Gibbs Sampler  

The Gibbs Sampler is a special case of an \ac{MCMC} algorithm, which is useful to approximate the combined distribution of two or more regressors in a multidimensional problem. 

The algorithm tries to find the approximate joint distribution and therefore the algorithm  runs through the sub-vectors of $\beta$ and draws each subset conditional on all other values. [@gelman_bayesian_2004]

\newpage

In the __monomvn__ package in __R__ [@gramacy_monomvn_2019] the Gibbs sampler for the  Bayesian \ac{LASSO} samples from the following representation of the Laplace distribution. @andrews_scale_1974

\begin{align} 
\label{eq:gibbs}
  \dfrac{a}{2}e^{-a |z|} = \int_{0}^{\infty} \dfrac{1}{2 \sqrt{\sigma^2}} e^{ -z^2 / (2s)} \; \dfrac{a^2}{2} e^{ -a^2 s /2} ds, \qquad a > 0 
\end{align}



### The full Model specification

The full model has the following hierarchical representation

\begin{align*}
  \pmb{y|\mu}, \pmb{X}, \pmb{\beta}, \sigma^2 & \sim N_n (\mu \pmb{1}_n + \pmb{X \beta}, \sigma^2  \pmb{I}_n)   \nonumber\\
  \pmb{\beta} | \sigma^2, \tau^2_1 , \ldots , \tau^2_p & \sim N_p (\pmb{A}^{-1} \pmb{X}^T \tilde{\pmb{y}}  , \sigma^2 \pmb{A}^{-1}) \qquad \text{with} \; \pmb{A} = \pmb{X}^T \pmb{X} + \pmb{D}^{-1}_{\tau} \nonumber \\
  \pmb{D}_{\tau} & = diag(\tau^2_1 , \ldots , \tau^2_p) \nonumber \\ 
  \sigma^2, \tau^2_1 , \ldots , \tau^2_p & \sim \pi \left( \sigma^2 \right) d \sigma^2 \prod_{j = 1}^{p} \dfrac{\lambda^2}{2}e^{- \lambda^2 \tau^2_j /2} d \tau^2_j \nonumber \\
  \sigma^2, \tau^2_1 , \ldots , \tau^2_p & > 0 \nonumber \\
\end{align*}

If $\tau^2_1 , \ldots , \tau^2_p$ gets integrated out of the conditional prior on $\pmb{\beta}$ , we get the form of \eqref{eq:la_bay_prior}. For $\sigma^2$ the inverse-gamma function of the form $\pi \left( \sigma^2 \right) = \dfrac{1}{\sigma^2}$ was implemented in the __monomvn__ package.     


\newpage
# Estimation and Results of the Models  


To compare the performances of the models all three models got, obviously, estimated with the same regressors.  We included as righthand variables: _log_wage_ , _age_, _height_cm_, _weight_kg_, _overall_, _potential_, _shooting_, _contract_valid_until_, _pace_, _passing_, _dribbling_,  and _defending_, so we have `r dim(train_x)[2]` explanatory variables to predict the response variable _log_value_. 

The reason we chose relatively few variables is that, on the one hand, we have enough variables, so it is very likely that some variables will not become significant, but on the other hand, we also have a better overview of the variables. 

Of course, this can lead to biases in the parameters, but the aim of the work is not to provide a causal interpretation of the explanatory variables, but rather to compare the methods.   


## Linear Model

\FloatBarrier
```{r structure and lm, echo = FALSE}

# the estimation of the models are in a seperate file, so that the compilation does not take to long, the models were stored in an additional file

######### linear model

#source("01_code/lm.R")

load(file = here::here('04_output/fit_lm.RData'))


knitr::kable(summary(fit_lm)$coef, digits = 4, caption = '\\label{tab:sum_lm} Summary of the linear model', format.args = list(decimal.mark = '.', big.mark = " "),   booktabs = TRUE, linesep = "")%>%
  kable_styling(latex_options = c("striped", "hold_position"))

#sum(summary(fit_lm)$coef[,4] > 0.05)
```
\FloatBarrier

In Table \ref{tab:sum_lm} one can see that only `r sum(summary(fit_lm)$coef[,4] > 0.05)` parameter _weight_kg_ is not significant to a 5 per cent level. The variable _overall_ has, naturally, the biggest (positive) impact on the _log_value (value)_, whereas _age_ has the biggest negative effect. 

\begin{align} 
\label{eq:t_test}
  t = \dfrac{\beta_i - 0}{se \left( \beta_i \right)} = \dfrac{\beta_i - 0}{ \dfrac{s}{\sqrt{n}} }  
\end{align}

Table \ref{tab:sum_lm} also shows that some coefficients are relatively small but still significant. However, a general problem with \ac{OLS} estimation is that with increasing sample size, many "coefficients" become significant, as one can in equation \ref{eq:t_test} . This is because the standard errors become smaller with increasing N, the t-statistic becomes larger, and the p-value smaller. [@royall_effect_1986] 

These coefficients (e.g.: _pace_, or _passing_ ) could be zero in the \ac{LASSO} or Bayesian \ac{LASSO} estimation because of the punishment term.   

## \acf{LASSO}


```{r , echo = FALSE}
 #### freq LASSO ####

 #source("01_code/glmnet.R")

 load(file = here::here('04_output/glmnet_lm.RData'))

 glmnet_coef <- as.matrix(coef(glmnet_fit))

 glmnet_coef_k <- round(glmnet_coef,6)
 glmnet_coef_k[glmnet_coef_k == 0] <- '-'

 knitr::kable(glmnet_coef_k, digits = 6, caption = '\\label{tab:sum_lasso} Summary of the LASSO ', format.args = list(decimal.mark = '.', big.mark = " "),  col.names = c('Estimate'),  booktabs = TRUE, linesep = "")%>%
   kable_styling(latex_options = c("striped", "hold_position"))

# sum(glmnet_coef == 0)
```

For the frequentists \ac{LASSO} we used the __cv.glmnet__ cross-validation function from the __glmnet__ package with $100$ folds to gain an estimate for $\lambda$. A $\lambda$ of `r round(LA_all$lambda.min,5)` minimized the mean cross-validated error. However, we used a lambda of  `r round(LA,5)` which is the largest $\lambda$ such that the $\lambda$ is in still within one standard error of the minimum.     @hastle_glmnet_2019

  
As one can see in Table \ref{tab:sum_lasso} there are considerable differences to the \ac{OLS} results. The \ac{LASSO} method has shrunk `r sum(glmnet_coef == 0)` parameters so much that they are no longer included in the model equation.  


It may be particularly noticeable, because it seems contra intuitive and the parameter had the biggest impact of all `r sum(glmnet_coef == 0)` excluded parameters in the linear model, that the variable _contract_valid_until_ is also not included in the model.  

Since LASSO does not only estimate regressors, but also selects them, no significance tests are needed.  


## Bayesian Lasso 

With the __blasso__ function of the __R__ package __monomvn__ it is possible to set the hyperparameters $\lambda$, for the penalty term, and  $\alpha \,  \text{and} \, \beta$, which are the shape and rate parameter for the prior. The $\lambda$ is in our case an empirical parameter which will be approximate through an updating Gibbs sampler. The algorithm uses the parameter of the previous sample. So iteration $k$ uses the Gibbs sampler with hyperparmeter $k-1$.
For the frequentists \ac{LASSO} the $\lambda$-parameter was `r round(LA, 5)`, so we decided to set  $\lambda = 10$,  since the first $25  \%$ of the \ac{MCMC} are not used for the estimation and the sampler convergence rather quickly.  [@gramacy_monomvn_2019] 

\FloatBarrier
\begin{align*} 
  \lambda^{k} = \sqrt{\dfrac{2p}{\displaystyle \sum_{j = 1}^{p} E_{\lambda^{(k-1)}} [\tau_j^2| \pmb{y}] }}
\end{align*}
\FloatBarrier
The expectations are replaced with averages from the previous Gibbs sampler. As @park_bayesian_2008 has shown any non-extreme starting value for $\lambda$ can be used. In the first setting we did not pass any parameters for $\alpha \, \text{or} \, \beta$. 



```{r , echo = FALSE}
 
 ### bayes LASSO ###
#source("01_code/bayes.R")
load(file = here::here('04_output/bayes_lasso_fit.RData'))

coef_lasso <- as.data.frame(cbind(beta1 = bayes_lasso_fit$beta[, "b.1"],
                                  beta2 = bayes_lasso_fit$beta[, "b.2"],
                                   beta3 = bayes_lasso_fit$beta[, "b.3"],
                                   beta4 = bayes_lasso_fit$beta[, "b.4"],
                                   beta5 = bayes_lasso_fit$beta[, "b.5"],
                                   beta6 = bayes_lasso_fit$beta[, "b.6"],
                                   beta7 = bayes_lasso_fit$beta[, "b.7"],
                                   beta8 = bayes_lasso_fit$beta[, "b.8"],
                                   beta9 = bayes_lasso_fit$beta[, "b.9"],
                                   beta10 = bayes_lasso_fit$beta[, "b.10"],                                                                       beta11 = bayes_lasso_fit$beta[, "b.11"],
                                   beta12 = bayes_lasso_fit$beta[, "b.12"],
                                   variance = bayes_lasso_fit$s2,
                                   lambda.square = bayes_lasso_fit$lambda2))

 coef_lasso_hyper <- as.data.frame(cbind(beta1 = bayes_lasso_fit_hyper$beta[, "b.1"],
                                   beta2 = bayes_lasso_fit_hyper$beta[, "b.2"],
                                   beta3 = bayes_lasso_fit_hyper$beta[, "b.3"],
                                   beta4 = bayes_lasso_fit_hyper$beta[, "b.4"],
                                   beta5 = bayes_lasso_fit_hyper$beta[, "b.5"],
                                   beta6 = bayes_lasso_fit_hyper$beta[, "b.6"],
                                   beta7 = bayes_lasso_fit_hyper$beta[, "b.7"],
                                   beta8 = bayes_lasso_fit_hyper$beta[, "b.8"],
                                   beta9 = bayes_lasso_fit_hyper$beta[, "b.9"],
                                   beta10 = bayes_lasso_fit_hyper$beta[, "b.10"],
                                   beta11 = bayes_lasso_fit_hyper$beta[, "b.11"],
                                   beta12 = bayes_lasso_fit_hyper$beta[, "b.12"],
                                   variance = bayes_lasso_fit_hyper$s2,
                                   lambda.square = bayes_lasso_fit_hyper$lambda2))

 BA <- cbind(colMedians(coef_lasso),
 matrixStats::colQuantiles(as.matrix(coef_lasso), probs = c(0.025, 0.975)))
 colnames(BA)[1] <- 'median'
 rownames(BA)[1:12] <- rownames(glmnet_coef_k)[2:13]


 knitr::kable(BA, digits = 6, caption = '\\label{tab:sum_bay} Summary of the Bayesian LASSO ', format.args = list(decimal.mark = '.', big.mark = " "),  booktabs = TRUE, linesep = "")%>%
   kable_styling(latex_options = c("striped"))#, "hold_position"))

# sum(BA[,2] * BA[,3] <= 0)

```


As one can see in Table \ref{tab:sum_bay} the _median_ for all regressors except, _weight_kg_, are unequal to zero, whereas for the frequentist \ac{LASSO} we had `r sum(glmnet_coef == 0)` coefficients which are directly excluded from the model, e.g. zero. 

```{r tabel_bayes_1, echo = FALSE}
 HY <- cbind(colMedians(coef_lasso_hyper),
 matrixStats::colQuantiles(as.matrix(coef_lasso_hyper), probs = c(0.025, 0.975)))
 colnames(HY)[1] <- 'median'
 rownames(HY)[1:12] <- rownames(glmnet_coef_k)[2:13]
 
# knitr::kable(HY, digits = 6, caption = '\\label{tab:sum_bay_hy} Summary of the  Bayessian LASSO with hyperpriors', format.args = list(decimal.mark = '.', big.mark = " "),  booktabs = TRUE, linesep = "")%>%
 #  kable_styling(latex_options = c("striped", "hold_position"))

# sum(HY[,2] * HY[,3] <= 0)
```

However, it is unlikely that for multidimensional Bayesian model the median for a parameter is zero, since the computation depends on a Gibbs sampler. The Gibbs sampler itself is a special algorithm in the class of MCMC algorithms, which tries to solve the problem of integral formation with the help of random numbers. Therefore it is very unlikely to shrink parameters directly to 0.  

If we instead look at the 95 \%  credible interval, which is the Bayesian equivalent to a confidence interval and the Bayesian equvivalent to a significant test, we find that `r sum(BA[,2] * BA[,3] <= 0)` of these intervals include the zero.  

\newpage
# Residual Analysis, \acf{RMSE} and 'Sensitive Analysis' 

The next step is to compare the quality of the models. First, we will take a look at the (distribution of the) residuals and after that we will calculate the out-of-sample \ac{RMSE} for the 2020 _FIFA_ data set. 


```{r RMSE, echo = FALSE}
rmse_lm <-modelr::rmse(fit_lm, test)

##own RMSE Function
own_rmse  <-  function(y_hat, y) {
   rm_se <- sqrt(sum( ( (y_hat - y)^2)/length(y_hat) ))
   return(rm_se)
}


### predicting y_hat LASSO

#prediction LASSO

y_hat_freq_las <- rep(glmnet_coef[1],length(test$log_wage))+
  glmnet_coef[which(row.names(glmnet_coef) == 'log_wage')]*test$log_wage+ 
  glmnet_coef[which(row.names(glmnet_coef) == 'age')]*test$age + 
  glmnet_coef[which(row.names(glmnet_coef) == 'height_cm')]*test$height_cm +
  glmnet_coef[which(row.names(glmnet_coef) == 'weight_kg')]*test$weight_kg +
  glmnet_coef[which(row.names(glmnet_coef) == 'overall')]*test$overall +
  glmnet_coef[which(row.names(glmnet_coef) == 'potential')]*test$potential +
  glmnet_coef[which(row.names(glmnet_coef) == 'shooting')]*test$shooting +
  glmnet_coef[which(row.names(glmnet_coef) == 'contract_valid_until')]*test$contract_valid_until +  
  glmnet_coef[which(row.names(glmnet_coef) == 'pace')]*test$pace +
  glmnet_coef[which(row.names(glmnet_coef) == 'passing')]*test$passing +
  glmnet_coef[which(row.names(glmnet_coef) == 'dribbling')]*test$dribbling +    
  glmnet_coef[which(row.names(glmnet_coef) == 'defending')]*test$defending   

rmse_freq_las <- own_rmse(y_hat_freq_las, test$log_value)



###predicting Bayesian Lasso

y_hat_bay_las <- rep(median(bayes_lasso_fit$mu), length(test$log_wage))+
  BA[1,1]*test$log_wage +
  BA[2,1]*test$age +
  BA[3,1]*test$height_cm +
  BA[4,1]**test$weight_kg +
  BA[5,1]*test$overall +
  BA[6,1]*test$potential +
  BA[7,1]*test$shooting +
  BA[8,1]*test$contract_valid_until +  
  BA[9,1]*test$pace +
  BA[10,1]*test$passing +
  BA[11,1]*test$dribbling +   
  BA[12,1]*test$defending   

rmse_bay_las <- own_rmse(y_hat_bay_las , test$log_value)

###predicting Bayesian Lasso with hyper perameter

y_hat_bay_las_hy <- rep(median(bayes_lasso_fit_hyper$mu), length(test$log_wage))+
  HY[1,1]*test$log_wage +
  HY[2,1]*test$age +
  HY[3,1]*test$height_cm +
  HY[4,1]**test$weight_kg +
  HY[5,1]*test$overall +
  HY[6,1]*test$potential +
  HY[7,1]*test$shooting +
  HY[8,1]*test$contract_valid_until +  
  HY[9,1]*test$pace +
  HY[10,1]*test$passing +
  HY[11,1]*test$dribbling +   
  HY[12,1]*test$defending   

rmse_bay_las_hyp <- own_rmse(y_hat_bay_las_hy , test$log_value)

load( file = here::here('04_output/fitted.RData'))

res_bayes <- (train_y - fitted_va_bayes)
colnames(res_bayes) <- 'resdiuals'

```


## Residual Analysis

Residuals are defined as the difference between the actual value and the predicted value of the model. As you can see from equation \eqref{eq:res}, negative residuals mean that the model overestimates the value and positive residuals mean that the model underestimates the value.   [@hayashi_econometrics_2000, p. 16]

\begin{align} 
\label{eq:res}
  \epsilon =  y_i - \hat{y}_i =  y_i - (  \beta_0  + \pmb{\beta}_i \pmb{X})
\end{align}

One crucial assumption of the linear regression is that the residuals are normally distributed with mean $0$ and constant variance $\sigma^2$. 

```{r, , fig2, echo = FALSE, fig.cap = " \\label{fig:res_bay} Plot of the Residuals vs Fitted Values for the Bayesian LASSO",  out.width = '100%',  fig.align = "left"}
  knitr::include_graphics(here::here('04_output/pl_bay_res.png'), auto_pdf = FALSE)
```

In Figure \ref{fig:res_bay} the residuals versus the fitted values were plotted and it appears that several assumptions are violated, on a first glance. On one hand it seems to be that there is a relationship between fitted values and residuals. On the other hand, there also seem to be clusters with different variances. The variance in the range between 10 and 13 seems to be larger than the variance between 15 and 18, which could be a sign for heteroscedasticity. 

Furthermore, the model seems to have a systematic estimation error for high values, all estimated values above 16 have a  naegative residuum, i.e. the model overestimates the value of the players. Generally, it can be said that a pattern can be recognized and the residuals do not appear distributed independently of each other. 

```{r, , fig_dens_2, echo = FALSE, fig.cap = " \\label{fig:den_res_bay} Distribution of the Residuals of the Bayesian LASSO",  out.width = '100%',  fig.align = "left"}
   knitr::include_graphics(here::here('04_output/pl_bay_res_dens.png'), auto_pdf = FALSE)
```

The distribution of the residuals also does not seem to be normally distributed with a mean value of 0. In Figure \ref{fig:den_res_bay} it seems that the left tail is much longer and wider than the right tail.

The empirical mean of the residuals is `r mean(as.matrix(res_bayes))`, which is significant different to zero at a one percent level with a t-static of `r round(t.test(as.matrix(res_bayes))$statistic, 2) ` and a p-value of `r formatC( t.test(as.matrix(res_bayes))$p.value , format = "e", digits = 2)`. For the residuals of the \ac{OLS} and the \ac{LASSO} model, the observations are the same, as shown in Figures \ref{fig:res_lm} and \ref{fig:res_lm_dens}  for the \ac{OLS} model and Figure \ref{fig:res_lasso} and \ref{fig:res_lasso_dens} for the \ac{LASSO} model in the Appendix.   

Since the \acf{OLS} method is our base method for the comparison and we are mainly interested in comparing the number of parameters included (or excluded) in (from) the model. Therefore, the \ac{OLS} model was estimated again with corrected standard errors for heteroscedasticity.

```{r ROBUST ,  echo = FALSE, }
robust_lm <-summary(robustbase::lmrob(log_value ~ ., data = train) )
```


First of all, we performed  a Breusch-Pagan test for heteroscedasticity  which was significant to a 5\% level with a BP score of `r round(lmtest::bptest(fit_lm)$statistic, 2)` and a  p-value of $\approx$ `r formatC(lmtest::bptest(fit_lm)$p.value, format = "e", digits = 2)` and the non-constant variance score test of Breusch-Pagan test was also significant to a 5% level test with a $\chi^2$ score of `r round(car::ncvTest(fit_lm)$ChiSquare, 4)` and a p-value of `r car::ncvTest(fit_lm)$p`.

With the corrected standard errors, `r sum(robust_lm$coefficients[,4]>0.05)` coefficients are no longer significant, which is `r sum(robust_lm$coefficients[,4]>0.05) - sum(summary(fit_lm)$coef[,4] > 0.05)` coefficient more as in the first \ac{OLS} model. 

## \acf{RMSE} of the Models 

As already mentioned in the introduction, we have taken the _FIFA_ data from _2020_ as test data. At least briefly, it should be noted here that the leap from the 2019 edition to the 2020 edition has added a further dimension. Between the two years there could be a trend, which could catch other effects like inflation, that increases player values without being explained by any of the variables in the model.  

```{r year trend , echo = FALSE }
com_fifa <- fifa_data%>% dplyr::select(vars,
                                       year)
trend_rob <- summary(robustbase::lmrob(log_value ~ ., data = com_fifa) )
```

In a quick check using a linear \ac{OLS} model containing the data for both years and a dummy for the different versions, we get a significant estimator of `r round( trend_rob$coefficients['year2020',1],4)`. This estimator is significant with corrected standard errors at a significance level of 5 percent with a t-value of `r round(trend_rob$coefficients['year2020',3],3)` and a p-value of `r formatC(trend_rob$coefficients['year2020',4] , format = "e", digits = 2)`. 

The significant dummy for the _versions_ does not automatically mean that there is an annual trend. The main thing to keep in mind is that we excluded a number of variables at the beginning and that these can now be included in the dummy as a bias. 


However, all three models have the same problem and therefore this should not play a major role in the RMSE's assessment of the models. The RMSE should therefore increase equally for all three models. The \acf{RMSE} is calculated as follows:

\begin{align*} 
  RMSE = \sqrt{ \dfrac{ \displaystyle \sum_{i = 1}^{N} \left( \hat{y}_i  - y_i \right)^2} {N}    }
\end{align*}

The \ac{RMSE} is a relative comparison measure that calculates the performance of different models using the same data set. If the \ac{RMSE} is determined with other data, as in this case with the 2020 data, it is called an out-of-sample RMSE.  

The RMSE for the frequentist linear model is `r round(rmse_lm, 6)`, which in itself is difficult to interpret.  For the (frequentist) LASSO, the RMSE is `r round( rmse_freq_las, 6 )`, which is a reduction of `r round(((rmse_freq_las - rmse_lm) / rmse_lm)*100 ,2)` percent compared to the base linear regression.  

For the Bayesian \ac{LASSO}, which was estimated with $12.500$ \ac{MCMC} iterations, $2.500$ of which were cut off at the beginning and were not used for the calculation. We get a \ac{RMSE} of `r round(rmse_bay_las,6)`, which is a change to \ac{OLS} of `r round(((rmse_bay_las - rmse_lm) / rmse_lm)*100 ,2)` percent. Compared to the frequentist \ac{LASSO} we get a reduction of the RMSE of `r round(((rmse_bay_las - rmse_freq_las) / rmse_freq_las)*100 ,2)` percent.   


The improvement in the \ac{RMSE} of the Bayesian model compared to the linear model is relatively small and is even smaller compared to the \ac{LASSO}. 
However, the computational effort for the small improvement is relatively high. The linear model (with corrected standard errors) was calculated within a few seconds. Whereas the frequentist \ac{LASSO} took only slightly longer, despite the cross-validation for the punishment parameter $\lambda$, the Bayesian \ac{LASSO} took almost 18 hours due to the 12.500 \ac{MCMC} iterations.

## Changing other  Hyperparameter

As @park_bayesian_2008 have described in their paper, instead of selecting $\lambda$ directly, one can also use a diffuse prior and only specify the parameters of the gamma distribution. We took the proposed parameters from @park_bayesian_2008 with $r = 1$ and $\delta = 1.78$.    

\FloatBarrier
```{r sensity , echo = FALSE }
 HY <- cbind(colMedians(coef_lasso_hyper),
 matrixStats::colQuantiles(as.matrix(coef_lasso_hyper), probs = c(0.025, 0.975)))
 colnames(HY)[1] <- 'median'
 rownames(HY)[1:12] <- rownames(glmnet_coef_k)[2:13]
  knitr::kable(HY, digits = 6, caption = '\\label{tab:sum_bay_hy_1} Summary of the  Bayessian LASSO with hyperpriors', format.args = list(decimal.mark = '.', big.mark = " "),  booktabs = TRUE, linesep = "")%>%
   kable_styling(latex_options = c("striped", "hold_position"))

# sum(HY[,2] * HY[,3] <= 0)
```
\FloatBarrier

As Table \ref{tab:sum_bay_hy_1} shows, the results have not really changed.  There are still `r sum(HY[,2] * HY[,3] <= 0)` parameters outside the 95 percent credibility interval and, also the size of the effects have hardly changed, which is mainly due to the many iterations of the \c{MCMC} algorithm.  Also, the $\lambda$-parameter differs only slightly , from `r formatC(BA[14,1], format = "e", digits = 2)` to `r formatC(HY[14,1], format = "e", digits = 2)`. The respective 95 percent credibility intervals overlap, which means that they are not differ significantly.  

\newpage

# Conclusion 

As we have shown in sections 5 and 6, the results of the Bayesian \ac{LASSO} are very similar to the results of the frequentist \ac{LASSO}. However, the Bayesian \ac{LASSO} performs a bit better, measured by the \ac{RMSE}, which does not seem to be substantially different. Our results are similar to those of @park_bayesian_2008, who also found no great difference between Bayesian LASSO and the normal one. However, @hans_bayesian_2009 found a reduction of the average prediction error from 16 to 36 percent with different dataets. 

The small difference in the methods can - of course - also be due to the underlying data, since we estimate and calculate the RMSE with only one data set. As we have seen from the analyses of the residuals and the distribution of the response variable, the data are not optimal either.  A possibility for further analysis would be a Box-Cox transformation of the data to see if the Bayesian LASSO has greater advantages. Another possibility would be to generate data with a Monte Carlo simulation and then let the different methods do the estimation again. In this way, certain problems - such as heteroskedasticity - can be controlled.


\newpage



# Appendix

$\quad$

```{r, , fig3, echo = FALSE, fig.cap = " \\label{fig:res_lm} Plot of the Residuals vs Fitted Values for the Linear Model",  out.width = '100%',  fig.align = "left"}
  knitr::include_graphics(here::here('04_output/pl_lin_res.png'), auto_pdf = FALSE)
```

```{r, , fig3_dens, echo = FALSE, fig.cap = " \\label{fig:res_lm_dens} Density Plot of the Residuals of the Linear Model",  out.width = '100%',  fig.align = "left"}
  knitr::include_graphics(here::here('04_output/pl_lm_res_dens.png'), auto_pdf = FALSE)
```

\newpage

```{r, , fig4, echo = FALSE, fig.cap = " \\label{fig:res_lasso} Plot of the Residuals vs Fitted Values for the LASSO Model",  out.width = '100%',  fig.align = "left"}
  knitr::include_graphics(here::here('04_output/pl_las_res.png'), auto_pdf = FALSE)
```

```{r, , fig4_dens, echo = FALSE, fig.cap = " \\label{fig:res_lasso_dens} Density Plot of the Residuals of the LASSO Model",  out.width = '100%',  fig.align = "left"}
  knitr::include_graphics(here::here('04_output/pl_las_res_dens.png'), auto_pdf = FALSE)
```



\newpage
