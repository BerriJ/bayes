\documentclass[12pt,a4paper]{article}
\usepackage{lmodern}

\usepackage{placeins}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[lmargin = 5cm,rmargin = 2.5cm,tmargin = 2.5cm,bmargin = 2.5cm]{geometry}

% Figure Placement:
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}

%%%% Jens %%%%
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{numprint}
\npthousandsep{\,}

%% citation setup
\usepackage{csquotes}

\usepackage[backend=biber, maxbibnames = 99, style = apa]{biblatex}
\setlength\bibitemsep{1.5\itemsep}
\addbibresource{R_packages.bib}
\bibliography{references.bib}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true, linktocpage = TRUE]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Jens Klenke},
            pdftitle={Bayes Seminar},
            colorlinks=true,
            citecolor=black,
            urlcolor=black,
            linkcolor=black,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Bayes Seminar}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
\subtitle{Seminar in Econometrics}
  \author{Jens Klenke}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{today}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

%% linespread settings

\usepackage{setspace}

\onehalfspacing

% Language Setup

\usepackage{ifthen}
\usepackage{iflang}
\usepackage[super]{nth}
\usepackage[ngerman, english]{babel}

%Acronyms
\usepackage[printonlyused, withpage, nohyperlinks]{acronym}
\usepackage{changepage}

% Multicols for the Title page
\usepackage{multicol}

\begin{document}

\selectlanguage{english}

%%%%%%%%%%%%%% Jens %%%%%
\numberwithin{equation}{section}


%\maketitle

\begin{titlepage}
  \noindent\begin{minipage}{0.6\textwidth}
	  \IfLanguageName{english}{University of Duisburg-Essen}{Universität Duisburg-Essen}\\
	  \IfLanguageName{english}{Faculty of Business Administration and Economics}{Fakultät für Wirtschaftswissensschaften}\\
	  \IfLanguageName{english}{Chair of Econometrics}{Lehrstuhl für Ökonometrie}\\
  \end{minipage}
	\begin{minipage}{0.4\textwidth}
	  \begin{flushright}
  	  \vspace{-0.5cm}
      \IfLanguageName{english}{\includegraphics*[width=5cm]{Includes/duelogo_en.png}}{\includegraphics*[width=5cm]{Includes/duelogo_de.png}}
	  \end{flushright}
	\end{minipage}
  \\
  \vspace{1.5cm}
  \begin{center}
  \huge{Bayes Seminar}\\
  \vspace{.25cm}
  \Large{Seminar in Econometrics}\\
  \vspace{0.5cm}
  \large{Seminar Paper}\\
  \vspace{1cm}
  \large{  \IfLanguageName{english}{Submitted to the Faculty of \\ Ökonometrie  \\at the \\University of Duisburg-Essen}{Vorgelegt der \\Fakultät für Wirtschaftswissenschaften der \\ Universität Duisburg-Essen}\\}
  \vspace{0.75cm}
  \large{\IfLanguageName{english}{from:}{von:}}\\
  \vspace{0.5cm}
  Jens Klenke\\
  \end{center}
  %\vspace{2cm}
  \vfill
  \hrulefill

  \noindent\begin{minipage}[t]{0.3\textwidth}
  \IfLanguageName{english}{Reviewer:}{Erstgutachter:}
  \end{minipage}
  \begin{minipage}[t]{0.7\textwidth}
  \hspace{1cm}Christoph Hanck
  \end{minipage}

  \noindent\begin{minipage}[t]{0.3\textwidth}
  \IfLanguageName{english}{Deadline:}{Abgabefrist:}
  \end{minipage}
  \begin{minipage}[t]{0.7\textwidth}
  \hspace{1cm}Jan.~17th 2020
  \end{minipage}

  \hrulefill

  \begin{multicols}{2}

  Name:

  Matriculation Number:

  E-Mail:

  Study Path:

  Semester:

  Graduation (est.):
 
  \columnbreak

  Jens Klenke

  3071594
  
  jens.klenke@stud.uni-due.de

  M.Sc. Economics

  \nth{5}

  Winter Term 2020

	\end{multicols}

\end{titlepage}

\newgeometry{top=2cm, left = 5cm, right = 2.5cm, bottom = 2.5cm}


\pagenumbering{Roman}
{
\hypersetup{linkcolor=black}

\setcounter{tocdepth}{3}
\tableofcontents
}

\newpage
\listoffigures
\addcontentsline{toc}{section}{List of Figures}

%\newpage
\listoftables
\addcontentsline{toc}{section}{List of Tables}

\section*{List of Abbreviations}
\addcontentsline{toc}{section}{List of Abbreviations}

\begin{adjustwidth}{1.5em}{0pt}

\begin{acronym}[dummyyyy]
 \acro{bagging}{Bootstrap Aggregation}
 \acro{LASSO}{Least Absolute Shrinkage and Selection Operator}
 \acro{OLS}{(ordinary least squares}
 \acro{pcr}{Principal Components Regression}
 \acro{pls}{Partial Least Squares}
 \acro{RMSE}{Root Mean Squared Error}
 \acro{MCMC}{Markov chain Monte Carlo} 
 \acro{i.i.d.}{independent and identically distributed}
 \acroplural{LRG}[LRG]{längefristige Refinanzierungsgeschäfte}

%Falls eine Abkürzung in der Mehrzahl nicht einfach auf "s" endet muss das speziell eingestellt werden.
% \acro{slmtA}{super lange mega tolle Abkürzung} %Einzahl
 %\acroplural{slmtA}[slmtAs]{super lange mega tolle Abkürzungen} %Mehrzahl
 \acro{dummyyyy}{dummyyy}
\end{acronym}

\end{adjustwidth}

\restoregeometry

\newpage
\pagenumbering{arabic} %Roman arabic

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In recent years, the \ac{LASSO} method of
\textcite{tibshirani_regression_1996} has emerged as an alternative to
ordinary least squares estimation. The success of the method is mainly
due to its ability to perform both variable selection and estimation. As
already Tibshirani pointed out in his original paper the standard
\ac{LASSO} model can be interpreted as a linear regression with a
Laplace prior. \textcite{park_bayesian_2008} where the first to
implement the Bayesian l\ac{LASSO} \textgreater\textgreater using a
conditional Laplace prior specification\textless\textless.

Our goal is to compare the result of the Bayesian \ac{LASSO} with normal
\ac{LASSO} method and an ordinary least square estimation. The focus is
particularly on the number of non-significant parameters in the linear
model or, in case of the \acp{LASSO} the parameters equal to zero.

Relevanz and structure

\newpage

\hypertarget{theory-of-bayesian-inference}{%
\section{Theory of Bayesian
inference}\label{theory-of-bayesian-inference}}

The Bayesian (inference) statistics based on the Bayes' theorem for
events.

\begin{align}
\label{eq:bayes_theorem}
  P(A | B) = \dfrac{P (B | A) P(A)}{P(B)}
\end{align}

For Bayesian statistics the event theorem gets \eqref{eq:bayes_theorem}
rewritten to apply it to densities. Where \(\pi (\theta)\) is the prior
distribution - which could be gained from prior research or knowledge,
\(f(y | \theta )\) is the likelihood function, and \(\pi (\theta| y)\)
is the posterior distribution, we then get the following.

\begin{align}
\label{eq:bayes_dens}
  \pi (\theta | y) = \dfrac{f(y | \theta) \pi(\theta)}{f(y)}
\end{align}

From \eqref{eq:bayes_dens} the advantages and disadvantages of Bayesian
statistics compared to frequentist statistics can directly be retrieved.
One major advantage is that the Bayesian approach can account for prior
knowledge and points out a philosophical difference to the frequentist
approach - that the obtained data stands not alone. Another, key
difference and advantage is that in the Bayesian world the computations
are made with distributions and this leads to a better information level
than just the computation of the first and second moment. The
computation of distributions is also the greatest disadvantages or more
neutral the biggest problem of the Bayesian approach because in high
dimensional problems the computation takes a lot of times or is
sometimes even not possible. A solution to that is that with newer and
better computers it is possible to simulate the integrals with a
\ac{MCMC} method. \autocite[p.~100]{ghosh_introduction_2006} PAGE
NUMBER!! \newpage

\hypertarget{data-description}{%
\section{Data description}\label{data-description}}

We collected the data from the online database platform \emph{kaggel.}
The dataset includes 6 years of data for all players who were included
in the soccer simulation game \emph{FIFA} from \emph{EA Sports}. We
decided to keep the data for 2019 and 2020, only. The Data for 2019
contains 17538 datapoints which will be used for the estimation of the
different models whereas the 2020 data with 18028 will be used to
compare the quality of the models with an out of sample \ac{RMSE}. Both
datasets consist of 104 variables which will not all be included in the
estimations. Some Variables are just an ID or different length of names
and URLs. \autocite{leone_fifa_2020}

A fundamental problem of the dataset consists as goalkeepers are
systematically rated differently than field players. Therefore, in the
subcategories of the variable \emph{overall} all field player categories
were assigned NAs for goalkeepers. Conversely, all field players have
NAs in all goalkeeper categories. Because the algorithm of \ac{LASSO} in
R cannot handle NAs they have been set to zero for all models.

It is not very realistic that a fielder has no values in the goalkeeper
categories and vice versa. However, it can be argued, at least for
outfield players, that goalkeeper attributes play no role in determining
market values. This argumentation does not seem to hold for goalkeepers,
at least passing can be assumed to be an influential variable for the
market value, because is an essential asset for the passing game if the
goalkeeper has possession of the ball. Nevertheless, due to the lack of
alternatives, all NAs have been replaced by Zero.

\begin{table}[!h]

\caption{\label{tab:data}\label{tab:sum} Summary of some important variables for the 2019 FIFA edition}
\centering
\begin{tabular}[t]{lllrlrr}
\toprule
 & year &  & N &   & mean & sd\\
\midrule
\rowcolor{gray!6}  value\_eur & 2019 &  & 17 538 &  & 2 473 043.68 & 5 674 963.22\\
 & 2020 &  & 18 028 &  & 2 518 484.58 & 5 616 359.21\\
\rowcolor{gray!6}  wage\_eur & 2019 &  & 17 538 &  & 10 085.87 & 22 448.70\\
 & 2020 &  & 18 028 &  & 9 584.81 & 21 470.29\\
\rowcolor{gray!6}  overall & 2019 &  & 17 538 &  & 66.23 & 7.01\\
 & 2020 &  & 18 028 &  & 66.21 & 6.95\\
\rowcolor{gray!6}  age & 2019 &  & 17 538 &  & 25.17 & 4.64\\
 & 2020 &  & 18 028 &  & 25.23 & 4.63\\
\rowcolor{gray!6}  potential & 2019 &  & 17 538 &  & 71.40 & 6.15\\
 & 2020 &  & 18 028 &  & 71.56 & 6.14\\
\bottomrule
\end{tabular}
\end{table}

As one can see in Table \ref{tab:sum} the differences between the
editions for the most important variables are considerably small. For
example, from 2019 to 2020 the mean player \emph{value} (response
variable) increased by 4.54e+04 which is about 1.8 per cent or 0.01
standard deviations. Similar results are observable for the probably
most important righthand variables \emph{wage} and \emph{overall} with a
difference in the means of -0.02 and -0.003 standard deviations between
2019 and 2020.

\begin{figure}
\centering
\includegraphics{term_paper_bayes_files/figure-latex/fig1-1.pdf}
\caption{\label{fig:hist_1} Histograms of player values and log player
values}
\end{figure}

As can be seen for the variable value in Figure \ref{fig:hist_1}, this
relatively strong right-skew is distributed, a similar pattern can be
observed for the variable wage. Since we also estimate a linear model,
and this often leads to non-normally distributed residuals, these were
logarithmized.

\newpage

\hypertarget{used-models}{%
\section{Used Models}\label{used-models}}

To compare the Bayesian \ac{LASSO} we will also analyse the data with a
linear multivariate model, and the frequentist \ac{LASSO}. We will start
with the linear model and then gradually modify the model equations
respectively the condition for estimating the parameters. So that the
coherences between the individual methods become clear.

All three methods have the common assumption that the relationship is
linear, at least in the parameters. The assumption seems stricter than
it is at first, because the data can be manipulated in such a way that
the relationship is linear after all. In our data, this was done by
logarithmization.

\hypertarget{linear-model}{%
\subsection{Linear Model}\label{linear-model}}

The frequentist multivariate regression model has the follwing model
equation.

\begin{align}
\label{eq:lm}
\pmb{Y = \beta_0 + X \beta} + \pmb{\epsilon}
\end{align}

Where \(\pmb{y}\) is the \(n \times 1\) response vector, \(\pmb{X}\) is
the \(n \times p\) matrix of regressors and, \(\pmb{\epsilon}\) is the
\(n \times 1\) vecotr of \ac{i.i.d.} errors with mean 0 and unknown
variance \(\sigma^2\).

The coefficient will be estimated by the ordinary least square method,
which means that \(\pmb{\beta}\) should be chosen so that the
\emph{Euclidean norm} \(\left( || \mathbf{y - X\beta} ||_2 \right)\) is
minimal. This yields in the conditon for the estimation of coefficients:

\begin{align}
\label{eq:lm_con}
 \hat{\pmb{\beta}} = \argmin_{ \pmb{\beta}} (\pmb{y - \beta_0 - X  \beta})^T (\pmb{y - \beta_0 - X  \beta})
\end{align}

\hypertarget{section}{%
\subsection{\texorpdfstring{\acf{LASSO}}{}}\label{section}}

In the \ac{LASSO} method the model equation is the same as the equation
for the multivariate but the condition for the optimization of the
estimators in equation \eqref{eq:lm_con} has an additional punishment
term. Which leads to the following optimazation.

\begin{align}
\label{eq:la_con}
\hat{\pmb{\beta}} = \argmin_{\pmb{\beta}}  \left( \pmb{y - X \beta} \right)^T \left( \pmb{y - X \beta} \right) + \lambda \sum_{i = 1}^{p} |\beta_j|
\end{align}

for some \(\lambda \geq 0\). This method is also often referred to as
\(L_1\) -penalized least squares estimation.

Already in his original paper \textcite{tibshirani_regression_1996} has
pointed out the possibility that his methods can also be interpreted in
a Bayesian way. The LASSO estimates can be considered as posterior mode
estimates with a double-exponential Laplace prior.

\hypertarget{bayesian-lasso}{%
\subsection{Bayesian Lasso}\label{bayesian-lasso}}

\textcite{park_bayesian_2008} considered a fully Bayesian approach using
a conditional Laplace prior of the form

\begin{align} 
\label{eq:la_bay_prior}
 \pi \left( \pmb{\beta} | \sigma^2 \right)   = \prod_{j = 1}^{p} \dfrac{\lambda}{2 \sqrt{\sigma^e}} e^{\dfrac{- \lambda |\beta_j |}{\sqrt{\sigma^2}}} 
\end{align}

\autocite{park_bayesian_2008}

\hypertarget{gibbs-sampler-and}{%
\subsubsection{Gibbs Sampler and}\label{gibbs-sampler-and}}

The Gibbs Sampler is a special case of an \ac{MCMC} algorithm, which is
useful to approximate the combianed distribution of two or more
regressors in a multidemsinoal problem.

The algortithm tries to find the approximate joint distribution and
therefore the algortihm runs through the subvectors \(\beta\) and draws
ach subset conditional on all other values.
\autocite{gelman_bayesian_2004}

Bayesian \ac{LASSO} the Gibbs sampler in the \textbf{monomvn} package in
\textbf{R} \autocite{gramacy_monomvn_2019} samples from the following
representation of the Laplace distribution.
\textcite{andrews_scale_1974}

\begin{align} 
\label{eq:gibbs}
  \dfrac{a}{2}e^{-a |z|} = \int_{0}^{\infty} \dfrac{1}{2 \sqrt{\sigma^2}} e^{ -z^2 / (2s)} \; \dfrac{a^2}{2} e^{ -a^2 s /2} ds, \qquad a > 0 
\end{align}

\hypertarget{the-full-model-specification}{%
\subsubsection{The full Model
specification}\label{the-full-model-specification}}

The full model has the following hierarchical representation

\begin{align*}
  \pmb{y|\mu}, \pmb{X}, \pmb{\beta}, \sigma^2 & \sim N_n (\mu \pmb{1}_n + \pmb{X \beta}, \sigma^2  \pmb{I}_n)   \nonumber\\
  \pmb{\beta} | \sigma^2, \tau^2_1 , \ldots , \tau^2_p & \sim N_p (\pmb{A}^{-1} \pmb{X}^T \tilde{\pmb{y}}  , \sigma^2 \pmb{A}^{-1}) \qquad \pmb{A} = \pmb{X}^T \pmb{X} + \pmb{D}^{-1}_{\tau} \nonumber \\
  \pmb{D}_{\tau} & = diag(\tau^2_1 , \ldots , \tau^2_p) \nonumber \\ 
  \sigma^2, \tau^2_1 , \ldots , \tau^2_p & \sim \pi \left( \sigma^2 \right) d \sigma^2 \prod_{j = 1}^{p} \dfrac{\lambda^2}{2}e^{- \lambda^2 \tau^2_j /2} d \tau^2_j \nonumber \\
  \sigma^2, \tau^2_1 , \ldots , \tau^2_p & > 0 \nonumber \\
\end{align*}

If \(\tau^2_1 , \ldots , \tau^2_p\) gets integreted out of the
conditional prior on \(\pmb{\beta}\) , we get the form of
\eqref{eq:la_bay_prior}. For \(\sigma^2\) the inverse-gamma function of
the form \(\pi \left( \sigma^2 \right) = \dfrac{1}{\sigma^2}\) was
implemented in the \textbf{monomvn} package.

\newpage

\hypertarget{estimation-and-results-of-the-models}{%
\section{Estimation and Results of the
Models}\label{estimation-and-results-of-the-models}}

To compare the performances of the models all three models got,
obviously, estimated with the same regressors. We included as righthand
variables: \emph{log\_wage} , \emph{age}, \emph{height\_cm},
\emph{weight\_kg}, \emph{overall}, \emph{potential}, \emph{shooting},
\emph{contract\_valid\_until}, \emph{pace}, \emph{passing},
\emph{dribbling}, and \emph{defending}, so we have 12 to predict the
response variable \emph{log\_value}.

\hypertarget{linear-model-1}{%
\subsection{Linear Model}\label{linear-model-1}}

\FloatBarrier
\begin{table}[!h]

\caption{\label{tab:structure and lm}\label{tab:sum_lm} Summary of the linear model}
\centering
\begin{tabular}[t]{lrrrr}
\toprule
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\midrule
\rowcolor{gray!6}  (Intercept) & -8.5970 & 2.9222 & -2.9420 & 0.0033\\
log\_wage & 0.0679 & 0.0025 & 26.8466 & 0.0000\\
\rowcolor{gray!6}  age & -0.1004 & 0.0008 & -119.6665 & 0.0000\\
height\_cm & 0.0012 & 0.0004 & 2.7140 & 0.0067\\
\rowcolor{gray!6}  weight\_kg & 0.0001 & 0.0004 & 0.3175 & 0.7508\\
overall & 0.2098 & 0.0008 & 266.5231 & 0.0000\\
\rowcolor{gray!6}  potential & -0.0059 & 0.0007 & -8.1962 & 0.0000\\
shooting & 0.0049 & 0.0003 & 17.7691 & 0.0000\\
\rowcolor{gray!6}  contract\_valid\_until & 0.0051 & 0.0014 & 3.5164 & 0.0004\\
pace & 0.0008 & 0.0002 & 3.6118 & 0.0003\\
\rowcolor{gray!6}  passing & 0.0019 & 0.0004 & 4.5131 & 0.0000\\
dribbling & -0.0016 & 0.0005 & -3.4678 & 0.0005\\
\rowcolor{gray!6}  defending & -0.0017 & 0.0002 & -10.6851 & 0.0000\\
\bottomrule
\end{tabular}
\end{table}
\FloatBarrier

In Table \ref{tab:sum_lm} one can see that only 1 parameter is not
significant to a 5 per cent level. The variable \emph{overall} has,
naturally, the biggest (positive) impact on the \emph{log\_value
(value)}, whereas \emph{age} has the biggest negative effect.

Table \ref{tab:sum_lm} also shows that some coefficients are relatively
small but still significant. However, a general problem with \ac{OLS}
estimation is that with increasing sample size, many
\enquote{coefficients} become significant. This is because the standard
errors become smaller with increasing N, the t-statistic becomes larger,
and the p-value smaller. These coefficients (e.g.: \emph{pace}, or
\emph{passing} ) could be zero in the \ac{LASSO} estimation because of
the punishment term.

\hypertarget{section-1}{%
\subsection{\texorpdfstring{\acf{LASSO}}{}}\label{section-1}}

For the frequentists \ac{LASSO} we used the \textbf{cv.glmnet}
cross-validation function from the \textbf{glmnet} package with 100
folds to gain \(\lambda\). A \(\lambda\) of 0.00261 minimized the mean
cross-validated error. However, we used a lambda of 0.01156 which is the
largest \(\lambda\) such that error is still within one standard error
of the minimum. \textcite{hastle_glmnet_2019}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-4}\label{tab:sum_lasso} Summary of the LASSO }
\centering
\begin{tabular}[t]{ll}
\toprule
  & Estimate\\
\midrule
\rowcolor{gray!6}  (Intercept) & 1.72337\\
log\_wage & 0.066023\\
\rowcolor{gray!6}  age & -0.089616\\
height\_cm & -\\
\rowcolor{gray!6}  weight\_kg & -\\
overall & 0.200689\\
\rowcolor{gray!6}  potential & 0.001541\\
shooting & 0.004659\\
\rowcolor{gray!6}  contract\_valid\_until & -\\
pace & -\\
\rowcolor{gray!6}  passing & -\\
dribbling & -\\
\rowcolor{gray!6}  defending & -0.000213\\
\bottomrule
\end{tabular}
\end{table}

As one can see in Table \ref{tab:sum_lasso} there are significant
differences to the linear system of equations. Lasso has shrunk 6
parameters so much that they are no longer included in the model
equation. It may be particularly noticeable, because it seems contra
intuitive and it had the biggest impact in the linear model from the 6
excluded parameters, that the variable \emph{contract\_valif\_until} is
also not represented in the model.

Since LASSO does not only estimate regressors, but also selects them, no
significance tests are needed

\newpage

\hypertarget{bayesian-lasso-1}{%
\subsection{Bayesian Lasso}\label{bayesian-lasso-1}}

\hypertarget{settings-of-the-hyperparameters}{%
\subsubsection{Settings of the
hyperparameters}\label{settings-of-the-hyperparameters}}

In the \textbf{blasso} function of the \textbf{R} package
\textbf{monomvn} it is possible to set the hyperparameters \(\lambda\),
for the penalty term, and \(\alpha \, \text{and} \, \beta\), which are
the shape and rate parameter for the prior. The \(\lambda\) is in our
case an empirical parameter which will be approximate trhough an
updating Gibbs sampler. The algorithm uses the parameter of the previous
sample. So iteration \(k\) uses the Gibbs sampler with hyperparmeter
\(k-1\). For the frequentists \ac{LASSO} the \(\lambda\)-parameter was
0.01156, so we decided to set \(\lambda = 10\), since the first
\(25 \%\) of the \ac{MCMC} are not used for the estimation and the
sampler convergence rather quickly. \autocite{gramacy_monomvn_2019}

\FloatBarrier

\begin{align*} 
  \lambda^{k} = \sqrt{\dfrac{2p}{\displaystyle \sum_{j = 1}^{p} E_{\lambda^{(k-1)}} [\tau_j^2| \pmb{y}] }}
\end{align*} \FloatBarrier The expectations are replaced with averages
from the previous Gibbs sampler. As \textcite{park_bayesian_2008} has
shown any non extrem starting value for \(\lambda\) can be used. In the
first setting we did not pass any parameters for
\(\alpha \, \text{and} \, \beta\).

\FloatBarrier

\begin{table}

\caption{\label{tab:unnamed-chunk-5}\label{tab:sum_bay} Summary of the Bayesian LASSO }
\centering
\begin{tabular}[t]{lrrr}
\toprule
  & median & 2.5\% & 97.5\%\\
\midrule
\rowcolor{gray!6}  log\_wage & 0.067554 & 0.063834 & 0.071904\\
age & -0.100611 & -0.102215 & -0.098671\\
\rowcolor{gray!6}  height\_cm & 0.001264 & 0.000000 & 0.002029\\
weight\_kg & 0.000000 & 0.000000 & 0.000000\\
\rowcolor{gray!6}  overall & 0.209992 & 0.208272 & 0.211500\\
potential & -0.006020 & -0.007445 & -0.004492\\
\rowcolor{gray!6}  shooting & 0.004856 & 0.004186 & 0.005391\\
contract\_valid\_until & 0.004777 & 0.000000 & 0.008346\\
\rowcolor{gray!6}  pace & 0.000714 & 0.000000 & 0.001135\\
passing & 0.001771 & 0.000000 & 0.002559\\
\rowcolor{gray!6}  dribbling & -0.001549 & -0.002438 & 0.000000\\
defending & -0.001596 & -0.001960 & -0.001072\\
\rowcolor{gray!6}  variance & 0.057742 & 0.056657 & 0.058963\\
lambda.square & 0.000124 & 0.000037 & 0.000356\\
\bottomrule
\end{tabular}
\end{table}

\FloatBarrier

As one can see in Tabel \ref{tab:sum_bay} the \emph{median} for all
regressors are unequal to zero, whereas for the frequentist \ac{LASSO}
we have 6 coefficient which are directly ecluded from the model,
e.g.~zero.

However, it is unlikely that for multidimensional bayesian model the
median for a parameter is zero, since the computation depends on a Gibbs
sampler. If we instead look at the 95 \% credible interval we finde that
6 of these intervall include the zero.

\newpage

\hypertarget{residual-analysis-and-sensitive-analysis}{%
\section{\texorpdfstring{Residual Analysis, \acf{RMSE} and
\enquote{Sensitive
Analysis}}{Residual Analysis,  and `Sensitive Analysis'}}\label{residual-analysis-and-sensitive-analysis}}

The next step is to compare the quality of the model. First we will take
a look at the (distribution of the) residuals and after that we will
calculate the out-of-sample \ac{RMSE} for the 2020 \emph{FIFA} data set.

\hypertarget{residual-analysis}{%
\subsection{Residual Analysis}\label{residual-analysis}}

Residues are defined as the difference between the predicted value of
the model and the actual value. As you can see from equation
\eqref{eq:res}, negative residuals mean that the model overestimates the
value and positive residuals mean that the model underestimates the
value. \autocite[p.~16]{hayashi_econometrics_2000}

\begin{align} 
\label{eq:res}
  \epsilon =  y_i - \hat{y}_i =  y_i - (  \beta_0  + \pmb{\beta}_i \pmb{X})
\end{align}

One crucial assumption of the linear regression is that the residuals
are normally distributed with mean \(0\) and constant variance
\(\sigma^2\).

\begin{figure}

\includegraphics[width=1\linewidth]{C:/Users/HP/Documents/GitHub/bayes/04_output/pl_bay_res} \hfill{}

\caption{ \label{fig:res_bay} Plot of the Residuals vs Fitted Values for the Bayesian LASSO}\label{fig:fig2}
\end{figure}

In Figure \ref{fig:res_bay} the residuals versus the fitted values were
plotted and it appears that several assumptions are violated, on a first
glance. On the other hand there seem to be clusters of different
variances. The variance in the range between 10 and 13 seems to be
larger than the variance between 15 and 18, which could be a sign for
heteroscedasticity.

Furthermore, the model seems to have a systematic estimation error at
high values, all values above 16 all residuals are negative, i.e.~the
model overestimates the value of the players. Generally it can be said
that a pattern can be recognized and the residuals do not appear
distributed independently of each other.

\begin{figure}

\includegraphics[width=1\linewidth]{C:/Users/HP/Documents/GitHub/bayes/04_output/pl_bay_res_dens} \hfill{}

\caption{ \label{fig:den_res_bay} Distribution of the Residuals of the Bayesian LASSO}\label{fig:fig_dens_2}
\end{figure}

The distribution of the residuals also does not seem to be normally
distributed with a mean value of 0. In Figure \ref{fig:den_res_bay} it
seems os that the left tail is distributed much longer and wider than
the right tail.

The empirical mean of the residuals is -0.0138247, which is significant
at a one percent level with a t-static of -7.63 and a p-value of
2.50e-14.

Verteilung der Residuen plotten

Structur bruch? Varianz wenig hohe werte systemastisch unterschätzt
ausreißer

\hypertarget{section-2}{%
\subsection{\texorpdfstring{\ac{RMSE}}{}}\label{section-2}}

As mentioned in the first Section we will use the data from \emph{2020}
as test data.

We used

The measurment to compare the results will be the \acf{RMSE}, which is
definied as:

\begin{align*} 
  RMSE = \sqrt{ \dfrac{ \displaystyle \sum_{i = 1}^{N} \left( \hat{y}_i  - y_i \right)^2} {N}    }
\end{align*}

\FloatBarrier
\begin{table}[!h]

\caption{\label{tab:sensity }\label{tab:sum_bay_hy_1} Summary of the  Bayessian LASSO with hyperpriors}
\centering
\begin{tabular}[t]{lrrr}
\toprule
  & median & 2.5\% & 97.5\%\\
\midrule
\rowcolor{gray!6}  log\_wage & 0.067405 & 0.061796 & 0.072935\\
age & -0.100695 & -0.102174 & -0.099112\\
\rowcolor{gray!6}  height\_cm & 0.001274 & 0.000000 & 0.002045\\
weight\_kg & 0.000000 & 0.000000 & 0.000446\\
\rowcolor{gray!6}  overall & 0.209832 & 0.208392 & 0.211174\\
potential & -0.005893 & -0.007389 & -0.004306\\
\rowcolor{gray!6}  shooting & 0.004792 & 0.004169 & 0.005301\\
contract\_valid\_until & 0.004703 & 0.000000 & 0.007556\\
\rowcolor{gray!6}  pace & 0.000497 & 0.000000 & 0.001049\\
passing & 0.001454 & 0.000000 & 0.002508\\
\rowcolor{gray!6}  dribbling & -0.000977 & -0.002296 & 0.000000\\
defending & -0.001553 & -0.001958 & -0.001094\\
\rowcolor{gray!6}  variance & 0.057671 & 0.056589 & 0.058846\\
lambda.square & 0.000087 & 0.000028 & 0.000340\\
\bottomrule
\end{tabular}
\end{table}
\FloatBarrier

\newpage

\hypertarget{discussion-and-further-research}{%
\section{Discussion and further
research}\label{discussion-and-further-research}}

Struckturbruch LaSt Part

\newpage

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\(\quad\)

\begin{figure}

\includegraphics[width=1\linewidth]{C:/Users/HP/Documents/GitHub/bayes/04_output/pl_lin_res} \hfill{}

\caption{ \label{fig:res_lm} Plot of the Residuals vs Fitted Values for the Linear Model}\label{fig:fig3}
\end{figure}

\begin{figure}

\includegraphics[width=1\linewidth]{C:/Users/HP/Documents/GitHub/bayes/04_output/pl_las_res} \hfill{}

\caption{ \label{fig:res_lasso} Plot of the Residuals vs Fitted Values for the LASSO Model}\label{fig:fig4}
\end{figure}

\newpage
\newpage
\renewcommand*{\mkbibnamefamily}[1]{\textbf{#1}}
\renewcommand*{\mkbibnamegiven}[1]{\textbf{#1}}
\renewcommand*{\mkbibnameprefix}[1]{\textbf{#1}}
\renewcommand*{\mkbibnamesuffix}[1]{\textbf{#1}}


\printbibliography[title=References]
\pagenumbering{arabic}


\newpage
\textbf{Eidesstattliche Versicherung}

\bigskip

Ich versichere an Eides statt durch meine Unterschrift, dass ich die vorstehende Arbeit selbständig und ohne fremde Hilfe angefertigt und alle Stellen, die ich wörtlich oder annähernd wörtlich aus Veröffentlichungen entnommen habe, als solche kenntlich gemacht habe, mich auch keiner anderen als der angegebenen Literatur oder sonstiger Hilfsmittel bedient habe. Die Arbeit hat in dieser oder ähnlicher Form noch keiner anderen Prüfungsbehörde vorgelegen.

\vspace{1cm}
\rule{0pt}{2\baselineskip} %
\par\noindent\makebox[2.25in]{\indent Essen, den \hrulefill} \hfill\makebox[2.25in]{\hrulefill}%
\par\noindent\makebox[2.25in][l]{} \hfill\makebox[2.25in][c]{Jens Klenke}%


\end{document}
