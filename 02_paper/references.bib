
@article{smith_nonparametric_1996,
	title = {Nonparametric regression using {Bayesian} variable selection},
	volume = {75},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0304407695017631},
	doi = {10.1016/0304-4076(95)01763-1},
	abstract = {This paper estimates an additive model semiparametrically, while automatically selecting the significant independent variables and the app{\textasciitilde}opriatc power transformation of the dependent variable. The nonlinear variables arc modeled as regression splincs, with significant knots selected fiom a large number of candidate knots. The estimation is made robust by modeling the errors as a mixture of normals. A Bayesian approach is used to select the significant knots, the power transformation, and to identify oatliers using the Gibbs sampler to curry out the computation. Empirical evidence is given that the sampler works well on both simulated and real examples and that in the univariate case it compares faw)rably with a kernel-weighted local linear smoother, The variable selection algorithm in the paper is substantially fasler than previous Bayesian variable sclcclion algorithms.},
	language = {en},
	number = {2},
	urldate = {2019-12-07},
	journal = {Journal of Econometrics},
	author = {Smith, Michael and Kohn, Robert},
	month = dec,
	year = {1996},
	pages = {317--343},
	file = {Smith und Kohn - 1996 - Nonparametric regression using Bayesian variable s.pdf:C\:\\Users\\HP\\Zotero\\storage\\7NQHVLU2\\Smith und Kohn - 1996 - Nonparametric regression using Bayesian variable s.pdf:application/pdf}
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2020-01-06},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288}
}

@misc{leone_fifa_2020,
	title = {{FIFA} 20 complete player dataset},
	url = {https://kaggle.com/stefanoleone992/fifa-20-complete-player-dataset},
	abstract = {18k+ players, 100+ attributes extracted from the latest edition of FIFA},
	language = {en},
	urldate = {2020-01-07},
	author = {Leone, Stefano},
	year = {2020},
	file = {Snapshot:C\:\\Users\\HP\\Zotero\\storage\\MKKKDZ9S\\fifa-20-complete-player-dataset.html:text/html}
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214508000000337},
	doi = {10.1198/016214508000000337},
	abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
	number = {482},
	urldate = {2020-01-11},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	keywords = {Empirical Bayes, Gibbs sampler, Hierarchical model, Inverse Gaussian, Linear regression, Penalized regression, Scale mixture of normals},
	pages = {681--686},
	file = {Full Text PDF:C\:\\Users\\HP\\Zotero\\storage\\BJCDZPW8\\Park und Casella - 2008 - The Bayesian Lasso.pdf:application/pdf;Snapshot:C\:\\Users\\HP\\Zotero\\storage\\IKK736KJ\\016214508000000337.html:text/html}
}